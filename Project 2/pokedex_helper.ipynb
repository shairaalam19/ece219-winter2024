{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Image dataset: https://www.kaggle.com/datasets/hlrhegemony/pokemon-image-dataset?resource=download\n",
    "##### Metainfo csv: https://github.com/lgreski/pokemonData/blob/master/Pokemon.csv\n",
    "##### You can speed up model inference on colab by changing Hardware accelerator in runtime type to any GPU option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets transformers numpy pandas Pillow matplotlib \n",
    "!pip install torch tqdm scipy\n",
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "!pip install plotly umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas' has no attribute 'Panel'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m softmax\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexpress\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpx\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_objects\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgo\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmanifold\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TSNE\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\plotly\\express\\__init__.py:15\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pd \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;124;03m\"\"\"\\\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03mPlotly express requires pandas to be installed.\"\"\"\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     )\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_imshow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m imshow\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_chart_types\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     scatter,\n\u001b[0;32m     18\u001b[0m     scatter_3d,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     51\u001b[0m     density_mapbox,\n\u001b[0;32m     52\u001b[0m )\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_core\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     56\u001b[0m     set_mapbox_access_token,\n\u001b[0;32m     57\u001b[0m     defaults,\n\u001b[0;32m     58\u001b[0m     get_trendline_results,\n\u001b[0;32m     59\u001b[0m     NO_COLOR,\n\u001b[0;32m     60\u001b[0m )\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\plotly\\express\\_imshow.py:11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m image_array_to_data_uri\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxarray\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     xarray_imported \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\xarray\\__init__.py:9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malignment\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m align, broadcast, broadcast_arrays\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m full_like, zeros_like, ones_like\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m concat, auto_combine\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomputation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m apply_ufunc, dot, where\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (register_dataarray_accessor,\n\u001b[0;32m     12\u001b[0m                               register_dataset_accessor)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\xarray\\core\\combine.py:9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malignment\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m align\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmerge\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvariable\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IndexVariable, Variable, as_variable\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvariable\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m concat \u001b[38;5;28;01mas\u001b[39;00m concat_vars\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\xarray\\core\\merge.py:17\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m---> 17\u001b[0m PANDAS_TYPES \u001b[38;5;241m=\u001b[39m (pd\u001b[38;5;241m.\u001b[39mSeries, pd\u001b[38;5;241m.\u001b[39mDataFrame, \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPanel\u001b[49m)\n\u001b[0;32m     19\u001b[0m _VALID_COMPAT \u001b[38;5;241m=\u001b[39m Frozen({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124midentical\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m     20\u001b[0m                         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mequals\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     21\u001b[0m                         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbroadcast_equals\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     22\u001b[0m                         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimal\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m     23\u001b[0m                         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno_conflicts\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m4\u001b[39m})\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbroadcast_dimension_size\u001b[39m(\n\u001b[0;32m     27\u001b[0m     variables: List[Variable],\n\u001b[0;32m     28\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOrderedDict[Any, int]\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'Panel'"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import clip\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from scipy.special import softmax\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv file and image paths to construct pokedex, use type_to_load=None to load all types, else use a list of types 1 to load\n",
    "def construct_pokedex(csv_path='Pokemon.csv', image_dir='./images/', type_to_load=None):\n",
    "    pokedex = pd.read_csv(csv_path)\n",
    "    image_paths = []\n",
    "\n",
    "    for pokemon_name in pokedex[\"Name\"]:\n",
    "        imgs = glob(f\"{image_dir}/{pokemon_name}/0.jpg\")\n",
    "        if len(imgs) > 0:\n",
    "            image_paths.append(imgs[0])\n",
    "        else:\n",
    "            image_paths.append(None)\n",
    "\n",
    "    pokedex[\"image_path\"] = image_paths\n",
    "    pokedex = pokedex[pokedex[\"image_path\"].notna()].reset_index(drop=True)\n",
    "\n",
    "    # only keep pokemon with distinct id\n",
    "    ids, id_counts = np.unique(pokedex[\"ID\"], return_counts=True)\n",
    "    ids, id_counts = np.array(ids), np.array(id_counts)\n",
    "    keep_ids = ids[id_counts == 1]\n",
    "\n",
    "    pokedex = pokedex[pokedex[\"ID\"].isin(keep_ids)].reset_index(drop=True)\n",
    "    pokedex[\"Type2\"] = pokedex[\"Type2\"].str.strip()\n",
    "    if type_to_load is not None:\n",
    "        pokedex = pokedex[pokedex[\"Type1\"].isin(type_to_load)].reset_index(drop=True)\n",
    "    return pokedex\n",
    "\n",
    "# load clip model\n",
    "def load_clip_model():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model, preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "    return model, preprocess, device\n",
    "\n",
    "# inference clip model on a list of image path\n",
    "def clip_inference_image(model, preprocess, image_paths, device):\n",
    "    image_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for img_path in tqdm(image_paths):\n",
    "            img = Image.open(img_path)\n",
    "            img_preprocessed = preprocess(img).unsqueeze(0).to(device)\n",
    "            image_embedding = model.encode_image(img_preprocessed).detach().cpu().numpy()\n",
    "            image_embeddings += [image_embedding]\n",
    "            \n",
    "    image_embeddings = np.concatenate(image_embeddings, axis=0)\n",
    "    image_embeddings /= np.linalg.norm(image_embeddings, axis=-1, keepdims=True)\n",
    "    return image_embeddings\n",
    "\n",
    "# inference clip model on a list of texts\n",
    "def clip_inference_text(model, preprocess, texts, device):\n",
    "    with torch.no_grad():\n",
    "        text_embeddings = model.encode_text(clip.tokenize(texts).to(device)).detach().cpu().numpy()\n",
    "    text_embeddings /= np.linalg.norm(text_embeddings, axis=-1, keepdims=True)\n",
    "    return text_embeddings\n",
    "\n",
    "# compute similarity of texts to each image\n",
    "def compute_similarity_text_to_image(image_embeddings, text_embeddings):\n",
    "    similarity = softmax((100.0 * image_embeddings @ text_embeddings.T), axis=-1)\n",
    "    return similarity\n",
    "\n",
    "# compute similarity of iamges to each text\n",
    "def compute_similarity_image_to_text(image_embeddings, text_embeddings):\n",
    "    similarity = softmax((100.0 * image_embeddings @ text_embeddings.T), axis=0)\n",
    "    return similarity\n",
    "\n",
    "# Use TSNE to project CLIP embeddings to 2D space\n",
    "def umap_projection(image_embeddings, n_neighbors=15, min_dist=0.1, metric='cosine'):\n",
    "    distance_matrix = np.zeros((image_embeddings.shape[0], image_embeddings.shape[0]))\n",
    "    for i in range(image_embeddings.shape[0]):\n",
    "        for j in range(image_embeddings.shape[0]):\n",
    "            if i == j:\n",
    "                distance_matrix[i, j] = 1\n",
    "            else:\n",
    "                distance_matrix[i, j] = np.dot(image_embeddings[i], image_embeddings[j])\n",
    "    distance_matrix = 1 - distance_matrix\n",
    "    reducer = TSNE(n_components=2, metric=\"precomputed\", init=\"random\", random_state=42)\n",
    "    visualization_data = reducer.fit_transform(distance_matrix)\n",
    "    return visualization_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "219",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
