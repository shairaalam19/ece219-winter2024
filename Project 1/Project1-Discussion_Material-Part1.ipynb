{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1\n",
    "\n",
    "Project 1 is about classification on text data, where you extract features from raw texts and try different classification approaches to classify them into topics.\n",
    "\n",
    "During this discussion, we introduce some key concepts and usages of relevant python packages to help you get started with your project.\n",
    "\n",
    "This explanation is mainly from different sections of the scikit-learn tutorial on text classification available at http://scikit-learn.org."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A short introduction to `Numpy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = [1,2,3]\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello' 'worlds']\n",
      "--------------------\n",
      "[1 2 3]\n",
      "--------------------\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "--------------------\n",
      "<U6 int64 float64\n",
      "--------------------\n",
      "(2, 3)\n"
     ]
    }
   ],
   "source": [
    "a = np.array(['hello', 'worlds']) # this a 1D array\n",
    "b = np.array([1, 2, 3])\n",
    "c = np.array([[1.0, 2.0, 3.0],[4.0, 5.0, 6.0]]) # this is a 2D array\n",
    "print(a)\n",
    "print('-' * 20)\n",
    "print(b)\n",
    "print('-' * 20)\n",
    "print(c)\n",
    "print('-' * 20)\n",
    "print(a.dtype, b.dtype, c.dtype)\n",
    "print('-' * 20)\n",
    "print(c.shape) # same member variable exists for Pandas it is shape, not shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.17127942, 0.71862073, 0.65462346, 0.85386897],\n",
       "       [0.74969474, 0.88460675, 0.63936699, 0.04346669],\n",
       "       [0.39354624, 0.65591155, 0.09739239, 0.97920701]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = np.random.rand(3,4) # Project 1 see GLoVe part\n",
    "d\n",
    "#type(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.71862073, 0.65462346],\n",
       "       [0.88460675, 0.63936699],\n",
       "       [0.65591155, 0.09739239]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[:,1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.17127942, 0.71862073])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[0,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 12, 14]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d + 10 # braodcasting\n",
    "a = [i + 10 for i in [1,2,4]] # don't recommend\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = np.ones([3, 2])\n",
    "f = np.zeros(e.shape)\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vstack([e, f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 0., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.hstack([e, f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3., 3.])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(e, axis=0) # summation across rows is axis = 0 and across col is axis = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2., 2.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(e, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4],\n",
       "       [5, 6]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 3.],\n",
       "       [4., 5.],\n",
       "       [6., 7.]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e + g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = np.array([[1],[1]])\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3],\n",
       "       [ 7],\n",
       "       [11]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(g, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "1. In the discussion, we work with “20 Newsgroups” dataset as an alternative to the custom dataset. Many of the functions will be the same. It is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups, each corresponding to a different topic.\n",
    "2. To manually load the data, you need to run this python code. <a href=\"https://www.dropbox.com/s/5oek8qbsge1y64b/fetch_data.py?dl=0\">link to fetch_data.py</a>\n",
    "3. Easiest way to load the data is to use the built-in dataset loader for 20 newsgroups from scikit-learn package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories = ['comp.graphics', 'comp.sys.mac.hardware']\n",
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
    "twenty_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.utils.Bunch'>\n",
      "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n"
     ]
    }
   ],
   "source": [
    "print(type(twenty_train))\n",
    "print(twenty_train.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: winstead@faraday.ece.cmu.edu (Charles Holden Winstead)\n",
      "Subject: ftp site for Radius software???\n",
      "Organization: Electrical and Computer Engineering, Carnegie Mellon\n",
      "\n",
      "Hey All,\n",
      "\n",
      "Does anyone know if I can ftp to get the newest version of Radiusware\n",
      "and soft pivot from Radius?  I bought a pivot monitor, but it has an\n",
      "old version of this software and won't work on my C650, and Radius said\n",
      "it would be 4-5 weeks until delivery.\n",
      "\n",
      "Thanks!\n",
      "\n",
      "-Chuck\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(twenty_train.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of category indices of the documents\n",
    "twenty_train.target # twenty_train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['comp.graphics', 'comp.sys.mac.hardware']\n"
     ]
    }
   ],
   "source": [
    "# so index 0 corresponds to 'comp.graphics'; 1 to 'comp.sys.mac.hardware'\n",
    "print(twenty_train.target_names) # twenty_train['target_names'] # instead look at the column titled root_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comp.sys.mac.hardware\n"
     ]
    }
   ],
   "source": [
    "print(twenty_train.target_names[twenty_train.target[0]]) # unique() in pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The files themselves are loaded in memory in the data attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1162\n",
      "1162\n",
      "1162\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(len(twenty_train.data)) # shape of a pandas dataframe is very useful especially in DL when you might lose control of your matrix dimensions\n",
    "print(len(twenty_train.filenames))\n",
    "print(len(twenty_train.target))\n",
    "print(len(twenty_train.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting features from text files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer\n",
    "Convert a collection of text documents to a matrix of token counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=1) # np.array is a class\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the detailed documentation of `CountVectorizer`, see\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "For regular expression:\n",
    "https://docs.python.org/2/library/re.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop words\n",
    "\n",
    "Words that are too common to be useful in classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'mill', 'wherein', 'whose', 'until', 'thereupon', 'become', 'whole', 'still', 'of', 'however', 'sixty', 'never', 'whom', 'describe', 'but', 'she', 'am', 'thick', 'bottom', 'down', 'get', 'all', 'none', 'will', 'towards', 'full', 'therein', 'or', 'somehow', 'couldnt', 'everyone', 'cry', 'an', 'beforehand', 're', 'have', 'is', 'bill', 'my', 'rather', 'enough', 'amount', 'fire', 'once', 'so', 'how', 'were', 'your', 'herein', 'six', 'throughout', 'nowhere', 'who', 'very', 'elsewhere', 'themselves', 'mostly', 'onto', 'toward', 'next', 'which', 'around', 'must', 'whereas', 'beside', 'less', 'back', 'anyway', 'co', 'yourselves', 'well', 'under', 'us', 'yours', 'seem', 'part', 'herself', 'cannot', 'found', 'between', 'due', 'whereafter', 'top', 'both', 'more', 'many', 'fifteen', 'interest', 'together', 'whoever', 'fifty', 'hence', 'thence', 'whether', 'may', 'neither', 'you', 'yourself', 'again', 'as', 'below', 'them', 'would', 'could', 'some', 'latterly', 'alone', 'except', 'twenty', 'keep', 'move', 'here', 'by', 'any', 'another', 'first', 'other', 'through', 'been', 'their', 'me', 'go', 'his', 'anyhow', 'can', 'although', 'though', 'we', 'be', 'because', 'everything', 'show', 'thereby', 'front', 'give', 'further', 'ever', 'whereby', 'cant', 'each', 'con', 'much', 'amongst', 'put', 'should', 'her', 'nine', 'eg', 'five', 'nobody', 'hereafter', 'everywhere', 'across', 'out', 'yet', 'whenever', 'former', 'hasnt', 'own', 'where', 'beyond', 'without', 'see', 'whatever', 'sincere', 'within', 'hers', 'three', 'take', 'itself', 'made', 'when', 'from', 'into', 'meanwhile', 'moreover', 'seemed', 'becomes', 'formerly', 'already', 'up', 'might', 'least', 'inc', 'namely', 'along', 'one', 'sometime', 'over', 'not', 'while', 'now', 'its', 'those', 'to', 'two', 'was', 'thus', 'nothing', 'then', 'hereupon', 'anything', 'before', 'off', 'since', 'during', 'i', 'others', 'call', 'last', 'de', 'serious', 'for', 'that', 'ours', 'anyone', 'several', 'behind', 'besides', 'un', 'nor', 'do', 'ltd', 'thin', 'whence', 'find', 'in', 'please', 'seeming', 'indeed', 'afterwards', 'about', 'too', 'these', 'fill', 'third', 'done', 'after', 'above', 'it', 'somewhere', 'at', 'the', 'whereupon', 'nevertheless', 'than', 'mine', 'empty', 'became', 'why', 'even', 'often', 'becoming', 'a', 'latter', 'else', 'same', 'him', 'has', 'otherwise', 'always', 'hereby', 'this', 'he', 'on', 'almost', 'twelve', 'being', 'per', 'four', 'they', 'wherever', 'among', 'amoungst', 'thru', 'via', 'either', 'ie', 'himself', 'our', 'someone', 'ten', 'eleven', 'with', 'and', 'etc', 'name', 'forty', 'anywhere', 'therefore', 'what', 'seems', 'perhaps', 'sometimes', 'are', 'such', 'noone', 'upon', 'if', 'detail', 'ourselves', 'only', 'thereafter', 'few', 'hundred', 'whither', 'every', 'no', 'something', 'myself', 'eight', 'side', 'system', 'had', 'most', 'there', 'also', 'against'})\n",
      "318\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "stop_words = text.ENGLISH_STOP_WORDS\n",
    "print(stop_words)\n",
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x16 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 27 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\n",
    "    'This is the first document. I was went to store.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "    'Arash Shadi and Pavan'\n",
    "]\n",
    "X = vectorizer.fit_transform(corpus) # you need a batch from the corpus to extract reasonable features especially in memory-less settings.\n",
    "X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and',\n",
       " 'arash',\n",
       " 'document',\n",
       " 'first',\n",
       " 'is',\n",
       " 'one',\n",
       " 'pavan',\n",
       " 'second',\n",
       " 'shadi',\n",
       " 'the',\n",
       " 'third',\n",
       " 'this']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature names\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1],\n",
       "       [0, 0, 1, 0, 1, 0, 0, 2, 0, 1, 0, 1],\n",
       "       [1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0],\n",
       "       [0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1],\n",
       "       [1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The converse mapping from feature name to column index is stored in the vocabulary_ attribute of the vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1162, 6075)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count_vect = CountVectorizer()\n",
    "#count_vect = CountVectorizer(stop_words='english')\n",
    "count_vect = CountVectorizer(stop_words='english', min_df=3, max_df=0.7)\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(774, 6075)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_counts = count_vect.transform(twenty_test.data)\n",
    "X_test_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '0010580b', '01', '02', '020', '0200', '03', '030', '04', '040', '05', '06', '060493101758', '0608', '07', '08', '09', '0953', '0x100']\n",
      "--------------------\n",
      "ssd\n",
      "1313\n"
     ]
    }
   ],
   "source": [
    "print(count_vect.get_feature_names()[:20])\n",
    "print('-' * 20)\n",
    "print(count_vect.get_feature_names()[5161])\n",
    "print(count_vect.vocabulary_.get('circuitry'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$TF\\times IDF(t,d)=tf(t,d)\\times idf(t)$$\n",
    "<hr>\n",
    "$$idf(t)=\\log(\\frac{n}{df(t)})+1$$\n",
    "\n",
    "- $tf(t, d)$: term frequency of term $t$ in the document $d$.\n",
    "\n",
    "\n",
    "- $idf(t)$: inverse document frequency of term $t$ across the document dataset.\n",
    "  - $df(t)$: # of documents that contain the term $t$.\n",
    "  - Intuition: words that appear in all documents are useless in classificaiton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1162, 6075)\n",
      "--------------------\n",
      "[[0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 2]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]]\n",
      "--------------------\n",
      "[[0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.0823884 ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.10357708 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "print(X_train_tfidf.shape)\n",
    "print('-' * 20)\n",
    "print(X_train_counts.toarray()[:30,:5])\n",
    "print('-' * 20)\n",
    "print(X_train_tfidf.toarray()[:30,:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a classifier\n",
    "\n",
    "Let's train a classifier to predict the category of a post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'He is an OS developer' => comp.sys.mac.hardware\n",
      "'OpenGL on the GPU is fast' => comp.graphics\n",
      "'I like Apple Mojave OS' => comp.sys.mac.hardware\n",
      "'This discussion was full of weird stuff.' => comp.graphics\n"
     ]
    }
   ],
   "source": [
    "docs_new = ['He is an OS developer', 'OpenGL on the GPU is fast', 'I like Apple Mojave OS', \"This discussion was full of weird stuff.\"]\n",
    "\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, twenty_train.target_names[category]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pholur/anaconda3/envs/goodreads/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1077: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "model = NMF(n_components=50, init='random', random_state=0)\n",
    "W_train = model.fit_transform(X_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1162, 50)\n"
     ]
    }
   ],
   "source": [
    "print(W_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 6075)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H = model.components_\n",
    "H.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [5.60343694e-03, 0.00000000e+00, 2.51869020e-05, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.86312774e-03],\n",
       "       [1.67247737e-02, 2.23575367e-02, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       ...,\n",
       "       [6.17835488e-03, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [2.27013999e-03, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'He is an OS developer' => comp.graphics\n",
      "'OpenGL on the GPU is fast' => comp.graphics\n",
      "'I like Apple Mojave OS' => comp.sys.mac.hardware\n"
     ]
    }
   ],
   "source": [
    "W_test = model.transform(X_new_tfidf)\n",
    "\n",
    "clf = MultinomialNB().fit(W_train, twenty_train.target)\n",
    "\n",
    "\n",
    "predicted = clf.predict(W_test)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, twenty_train.target_names[category]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 6.01507454e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.87760905e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.47126358e-03, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.23838481e-03,\n",
       "        0.00000000e+00, 2.40511541e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 3.38612023e-03, 3.13733187e-03,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        9.69731116e-04, 0.00000000e+00, 2.20627340e-03, 0.00000000e+00,\n",
       "        2.16066187e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 3.74761017e-03, 0.00000000e+00, 6.02403561e-04,\n",
       "        0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.25292535e-03,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.40451066e-03, 0.00000000e+00, 0.00000000e+00, 1.48382271e-03,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.73743549e-03, 0.00000000e+00, 1.03226303e-03, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 4.82842545e-02, 0.00000000e+00,\n",
       "        3.05960616e-03, 4.48620108e-05, 4.06349113e-03, 0.00000000e+00,\n",
       "        8.06322603e-04, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 50)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goodreads",
   "language": "python",
   "name": "goodreads"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
