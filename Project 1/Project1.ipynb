{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1: End-to-End Pipeline to Classify News Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "### 1.1 Overview How many rows (samples) and columns (features) are present in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('Project1-ClassificationDataset.csv')\n",
    "\n",
    "num_rows, num_cols = df.shape\n",
    "\n",
    "print(f\"Number of rows (samples): {num_rows}\")\n",
    "print(f\"Number of cols (features): {num_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) The total number of alpha-numeric characters per data point (row) in the feature full text: i.e count on the x-axis and frequency on the y-axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your CSV file into a DataFrame\n",
    "file_path = 'Project1-ClassificationDataset.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# (a) Histogram for the total number of alpha-numeric characters per data point\n",
    "df['Total_AlphaNumeric_Count'] = df['full_text'].apply(lambda x: sum(c.isalnum() for c in str(x)))\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(df['Total_AlphaNumeric_Count'], bins=20, edgecolor='black')\n",
    "plt.title('Histogram of Total Alpha-Numeric Characters per Data Point')\n",
    "plt.xlabel('Count of Alpha-Numeric Characters')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) The column leaf label – class on the x-axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (b) Histogram for the actual column leaf label – class\n",
    "leaf_label_column = 'leaf_label'  \n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(df[leaf_label_column], bins=20, edgecolor='black')\n",
    "plt.title('Histogram of Actual Column Leaf Label – Class')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) The column root label – class on the x-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (c) Histogram for the actual column root label – class\n",
    "root_label_column = 'root_label'  \n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(df[root_label_column], bins=3, edgecolor='black')\n",
    "plt.title('Histogram of Actual Column Root Label – Class')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Interpret Plots: Provide qualitative interpretations of the histograms.\n",
    "\n",
    "** incomplete **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Histogram of Alpha Numeric Characters per Data Point\n",
    "- Histogram reveals the distribution of the total number of alpha-numeric characters in the 'full_text' feature for each data point\n",
    "\n",
    "(b) Histogram of Leaf Label Class\n",
    "- b\n",
    "\n",
    "(c) Histogram of Root Label Class\n",
    "- c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Splitting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report the number of training and testing samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(df[[\"full_text\",\"root_label\"]], test_size=0.2)\n",
    "\n",
    "num_training_samples = len(train)\n",
    "num_testing_samples = len(test)\n",
    "\n",
    "print(f\"Number of training samples: {num_training_samples}\")\n",
    "print(f\"Number of testing samples: {num_testing_samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Feature Extraction (Lemmatize and Vectorize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Cleaning each data sample. This function helps remove many but not all HTML artefacts from the crawler’s output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean(text):\n",
    "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    texter = re.sub(r\"<br />\", \" \", text)\n",
    "    texter = re.sub(r\"&quot;\", \"\\\"\",texter)\n",
    "    texter = re.sub('&#39;', \"\\\"\", texter)\n",
    "    texter = re.sub('\\n', \" \", texter)\n",
    "    texter = re.sub(' u ',\" you \", texter)\n",
    "    texter = re.sub('`',\"\", texter)\n",
    "    texter = re.sub(' +', ' ', texter)\n",
    "    texter = re.sub(r\"(!)\\1+\", r\"!\", texter)\n",
    "    texter = re.sub(r\"(\\?)\\1+\", r\"?\", texter)\n",
    "    texter = re.sub('&amp;', 'and', texter)\n",
    "    texter = re.sub('\\r', ' ',texter)\n",
    "    clean = re.compile('<.*?>')\n",
    "    texter = texter.encode('ascii', 'ignore').decode('ascii')\n",
    "    texter = re.sub(clean, '', texter)\n",
    "    if texter == \"\":\n",
    "        texter = \"\"\n",
    "    return texter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the clean(text) function to clean the crawler's output and remove most HTML artefacts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['full_text'] = train['full_text'].map(clean)\n",
    "test['full_text'] = test['full_text'].map(clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Create the CountVectorizer with \"english\" stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer(min_df=3, stop_words=\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3, 3.4, 3.5 Perform lemmatization with nltk.wordnet.WordNetLemmatizer and pos tag and Exclude terms that are numbers (e.g. “123”, “-45”, “6.7” etc.) using min_df=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "\n",
    "# nltk.download()\n",
    "wnl = nltk.wordnet.WordNetLemmatizer()\n",
    "\n",
    "# Penn Trebank to wordnet \n",
    "# In the wordnet (ADJ, ADJ_SAT, ADV, NOUN, VERB = 'a', 's', 'r', 'n', 'v', POS_LIST = [NOUN, VERB, ADJ, ADV])\n",
    "def penn2morphy(penntag):\n",
    "    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n",
    "    morphy_tag = {'NN':'n', 'JJ':'a',\n",
    "                  'VB':'v', 'RB':'r'}\n",
    "    try:\n",
    "        return morphy_tag[penntag[:2]]\n",
    "    except:\n",
    "        return 'n'\n",
    "\n",
    "# Lemmatize entire data (data made up of sentences. Lemmatize ever word in every sentence and return the lemmatized data)\n",
    "def lemmatize(data):\n",
    "    lemmatized_data = [] \n",
    "    for text in data:\n",
    "        # print(\"text\": text)\n",
    "        # Return a tokenized copy of text \n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        # print(tokens)\n",
    "        \n",
    "        # tags given list of tokens \n",
    "        tagged = pos_tag(tokens)\n",
    "        # print(tagged)\n",
    "\n",
    "        # lemmatize text excluding numbers \n",
    "        lemmatized_words = []\n",
    "        for word, tag in tagged:\n",
    "            # print(\"word: \" word)\n",
    "            if not re.match(r'^\\d+(\\.\\d+)?$', word): # double check this\n",
    "                lemmatized_word = wnl.lemmatize(word.lower(), pos=penn2morphy(tag))\n",
    "                lemmatized_words.append(lemmatized_word)\n",
    "            \n",
    "        lemmatized_sentence = ' '.join(lemmatized_words)\n",
    "        \n",
    "        # Add whole sentence back to the lemmatize_data array \n",
    "        lemmatized_data.append(lemmatized_sentence)\n",
    "\n",
    "    return lemmatized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize train and test datasets \n",
    "train_lemmatized = lemmatize(train['full_text'])\n",
    "test_lemmatized = lemmatize(test['full_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the lemmatized train and test datasets \n",
    "X_train_counts = count_vect.fit_transform(train_lemmatized)\n",
    "X_test_counts = count_vect.transform(test_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# TF-IDF \n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
    "\n",
    "print(\"TD-TFIDF-processed Train Matrix Shape: \", X_train_tfidf.shape)\n",
    "print(\"TD-TFIDF-processed Test Matrix Shape: \", X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Answer Questions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) What are the pros and cons of lemmatization versus stemming? How do these processes affect the dictionary size?\n",
    "**Lemmatization:** \n",
    "It is a technique that involves reducing words to their root form, which is called lemma. \n",
    "\n",
    "Pros\n",
    "1) The base/root words are actual words with meanings. So after lemmatization we do not lose the meaning of a word. \n",
    "2) In lemmatization, since we retain the meaning of the words, the accuracy is better than stemming.\n",
    "\n",
    "Cons\n",
    "1) It is slower than stemming. \n",
    "2) To implement lemmatization we need to have some linguistic understanding. \n",
    "\n",
    "**Stemming:** \n",
    "Stemming removes suffixes and prefixes from words, so that we can get a common representation called the stem of a word. \n",
    "\n",
    "Pros\n",
    "1) Stemming is less computationally expensive and runs faster. Stemming is easy to implement as it does not take into account the meanings of words.\n",
    "2)  Stemming helps in reducing the dictionary size. \n",
    "\n",
    "Cons\n",
    "1) The words we obtain after stemming may not be valid words with some meaning. \n",
    "2) The acccuracy might be lesser as compared to lemmatization. \n",
    "\n",
    "**Dictionary size:**\n",
    "Stemming gives a smaller dictionary size as it does not consider the meaning of a word, it just reduces the word by removing prefixes or suffixes. Whereas, lemmatization takes into account the meaning and results in a larger dictionary size. \n",
    "\n",
    "#### (b) min df means minimum document frequency. How does varying min df change the TF-IDF matrix?\n",
    "The min_df parameter is used to remove the terms that are very infrequent. If min_df = 3 that means ignore the terms that appear in less than 3 documents. \n",
    "\n",
    "\n",
    "#### (c) Should I remove stopwords before or after lemmatizing? Should I remove punctuations before or after lemmatizing? Should I remove numbers before or after lemmatizing? Hint: Recall that the full sentence is input into the Lemmatizer and the lemmatizer is tagging the position of every word based on the sentence structure.\n",
    "- The stopwords are removed after applying the lemmatizer. The punctuations are removed after lemmatizing, and the removing the numbers is also removed after lemmatizing.\n",
    "\n",
    "#### (d) Report the shape of the TF-IDF-processed train and test matrices. The number of rows should match the results of Question 2. The number of columns should roughly be in the order of k×(10^3). This dimension will vary depending on your exact method of cleaning and lemmatizing and that is okay.\n",
    "- The shape of the TD-IDF-processed train matrix is (2780, 13754). The shape of the TD-IDF-processed test matrix is (696, 13754). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Dimensionality Reduction (LSI and NMF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 Plot the explained variance ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT VALIDATED\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Define values of k\n",
    "k_values = [1, 10, 50, 100, 200, 500, 1000, 2000]\n",
    "\n",
    "# Plot explained variance ratio for different values of k\n",
    "explained_variances = []\n",
    "\n",
    "for k in k_values:\n",
    "    lsi = TruncatedSVD(n_components=k, random_state=0)\n",
    "    lsi.fit_transform(X_train_tfidf)\n",
    "    explained_variances.append(lsi.explained_variance_ratio_.sum())\n",
    "\n",
    "plt.plot(k_values, explained_variances, marker='o')\n",
    "plt.xlabel('Number of Components (k)')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Explained Variance Ratio vs. Number of Components (LSI)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 What does the explained variance ratio plot look like? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "incomplete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.3 What does the plot’s concavity suggest?\n",
    "incomplete "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 Calculate the reconstruction residual MSE error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSI\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "lsi = TruncatedSVD(n_components=50, random_state=42)\n",
    "X_train_lsi = lsi.fit_transform(X_train_tfidf)\n",
    "X_test_lsi = lsi.transform(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NMF\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "nmf = NMF(n_components=50, random_state=42)\n",
    "X_train_NMF = nmf.fit_transform(X_train_tfidf)\n",
    "X_test_NMF = nmf.transform(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSI Reconstructed Residual MSE Error \n",
    "USigmaVt = lsi.inverse_transform(X_train_lsi)\n",
    "residual_MSE_lsi = np.sum(np.array(X_train_tfidf - USigmaVt)**2)\n",
    "\n",
    "# NMF Reconstructed Residual MSE Error \n",
    "WH = np.dot(X_train_NMF, nmf.components_)\n",
    "residual_MSE_nmf = np.sum(np.array(X_train_tfidf - WH)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2 Which one is larger, NMF or LSI and why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if residual_MSE_lsi > residual_MSE_nmf: \n",
    "    print(\"LSI has a larger residual MSE error than NMF.\")\n",
    "elif residual_MSE_lsi < residual_MSE_nmf: \n",
    "    print(\"NMF has a larger residual MSE error than LSI.\")\n",
    "else: \n",
    "    print(\"NMF has the same residual MSE error as LSI.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "incomplete (explain why)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: Classification Algorithms (Hard Margin and Soft Margin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.1 Train one SVM with γ = 1000 (hard margin), another with γ = 0.0001 (soft margin)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# dummy_train, dummy_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# dummy_train['full_text'] = dummy_train['full_text'].map(clean)\n",
    "# dummy_test['full_text'] = dummy_test['full_text'].map(clean)\n",
    "\n",
    "reverse_row_to_class = {'climate': 1, 'non-climate': 0}\n",
    "\n",
    "train['r_label'] = train['root_label'].apply(lambda x: reverse_row_to_class[x] if x in reverse_row_to_class else -1)\n",
    "test['r_label'] = test['root_label'].apply(lambda x: reverse_row_to_class[x] if x in reverse_row_to_class else -1)\n",
    "\n",
    "\n",
    "# Binary classification labels for training set\n",
    "Y_train = train['r_label']\n",
    "\n",
    "# Binary classification labels for testing set\n",
    "Y_test = test['r_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard Margin\n",
    "hard_margin = SVC(C=1000, kernel='linear', random_state=42)\n",
    "\n",
    "hard_margin = hard_margin.fit(X_train_lsi, Y_train)\n",
    "\n",
    "Y_test_pred_hard = hard_margin.predict(X_test_lsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soft Margin\n",
    "soft_margin = SVC(C=0.0001, kernel='linear', random_state=42)\n",
    "\n",
    "soft_margin = soft_margin.fit(X_train_lsi, Y_train)\n",
    "\n",
    "Y_test_pred_soft = soft_margin.predict(X_test_lsi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.2 Plot the ROC curve, report the confusion matrix and calculate the accuracy, recall, precision and F-1 score of both SVM classifiers on the testing set. Which one performs better? What about for γ = 100000?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT VALIDATED\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),horizontalalignment=\"center\",color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve\n",
    "def plot_roc(fpr, tpr, title=\"ROC Curve\"):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    roc_auc = auc(fpr,tpr)\n",
    "\n",
    "    ax.plot(fpr, tpr, lw=2, label= 'area under curve = %0.4f' % roc_auc)\n",
    "\n",
    "    ax.grid(color='0.7', linestyle='--', linewidth=1)\n",
    "\n",
    "    ax.set_xlim([-0.1, 1.1])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate',fontsize=15)\n",
    "    ax.set_ylabel('True Positive Rate',fontsize=15)\n",
    "\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    ax.set_title(title, fontsize=16)\n",
    "\n",
    "    for label in ax.get_xticklabels()+ax.get_yticklabels():\n",
    "        label.set_fontsize(15)\n",
    "\n",
    "def fit_predict_and_plot_roc(pipe, train_data, train_label, test_data, test_label, title=\"ROC Curve\"):\n",
    "    pipe.fit(train_data, train_label)\n",
    "\n",
    "    if hasattr(pipe, 'decision_function'):\n",
    "        prob_score = pipe.decision_function(test_data)\n",
    "        fpr, tpr, _ = roc_curve(test_label, prob_score)\n",
    "    else:\n",
    "        prob_score = pipe.predict_proba(test_data)\n",
    "        fpr, tpr, _ = roc_curve(test_label, prob_score[:,1])\n",
    "\n",
    "    plot_roc(fpr, tpr, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard Margin \n",
    "print(\"Hard Margin\")\n",
    "\n",
    "# ROC Curve \n",
    "hard_m = SVC(C=1000, kernel='linear', random_state=42)\n",
    "fit_predict_and_plot_roc(hard_m, X_train_lsi, Y_train, X_test_lsi, Y_test, \"Hard Margin ROC Curve\")\n",
    "\n",
    "# Confusion Matrix \n",
    "print('Confusion Matrix:\\n', confusion_matrix(Y_test, Y_test_pred_hard))\n",
    "class_names = ['Computer \\n Technology', 'Recreational \\n Activity']\n",
    "hard_cm = confusion_matrix(Y_test, Y_test_pred_hard) \n",
    "plt.figure(); plot_confusion_matrix(hard_cm, classes=class_names, title='Hard SVM Confusion Matrix')\n",
    "\n",
    "# Statistics  \n",
    "print('Accuracy:', accuracy_score(Y_test, Y_test_pred_hard))\n",
    "print('Recall:', recall_score(Y_test, Y_test_pred_hard))\n",
    "print('Precision:', precision_score(Y_test, Y_test_pred_hard))\n",
    "print('F-1 Score:', f1_score(Y_test, Y_test_pred_hard))\n",
    "\n",
    "# γ = 100000\n",
    "hard_margin = SVC(C=100000, kernel='linear', random_state=42)\n",
    "\n",
    "hard_margin = hard_margin.fit(X_train_lsi, Y_train)\n",
    "\n",
    "Y_test_pred_hard = hard_margin.predict(X_test_lsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soft Margin \n",
    "print(\"Soft Margin\")\n",
    "\n",
    "# ROC Curve \n",
    "soft_m = SVC(C=0.0001, kernel='linear', random_state=42)\n",
    "fit_predict_and_plot_roc(soft_m, X_train_lsi, Y_train, X_test_lsi, Y_test, \"Soft Margin ROC Curve\")\n",
    "\n",
    "# Confusion Matrix \n",
    "print('Confusion:\\n', confusion_matrix(Y_test, Y_test_pred_soft))\n",
    "class_names = ['Computer \\n Technology', 'Recreational \\n Activity']\n",
    "soft_cm = confusion_matrix(Y_test, Y_test_pred_hard) \n",
    "plt.figure(); plot_confusion_matrix(soft_cm, classes=class_names, title='Soft SVM Confusion Matrix')\n",
    "\n",
    "# Statistics  \n",
    "print('Accuracy:', accuracy_score(Y_test, Y_test_pred_soft))\n",
    "print('Recall:', recall_score(Y_test, Y_test_pred_soft))\n",
    "print('Precision:', precision_score(Y_test, Y_test_pred_soft))\n",
    "print('F-1 Score:', f1_score(Y_test, Y_test_pred_soft))\n",
    "\n",
    "# γ = 100000\n",
    "svm_soft = SVC(C=100000, kernel='linear', random_state=42)\n",
    "\n",
    "svm_soft = svm_soft.fit(X_train_lsi, Y_train)\n",
    "\n",
    "Y_test_pred_soft = svm_soft.predict(X_test_lsi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.3 What happens for the soft margin SVM? Why is the case? Analyze in terms of the confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "incomplete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.4 Does the ROC curve reflect the performance of the soft-margin SVM? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "incomplete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.1 Use cross-validation to choose γ (use average validation 3 accuracy to compare): Using a 5-fold cross-validation, find the best value of the parameter γ in the range {10k| − 3 ≤ k ≤ 6, k ∈ Z}. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Cross Validation\n",
    "gamma_vals = [10**k for k in range(-3, 7)]\n",
    "\n",
    "best_gamma = None\n",
    "best_accuracy = 0\n",
    "\n",
    "# Iterate through gamma values \n",
    "for gamma in gamma_vals:\n",
    "    svc = SVC(kernel='linear', C=gamma).fit(X_train_lsi, Y_train)\n",
    "    scores = cross_val_score(svc, X_train_lsi, Y_train,cv=5, scoring='accuracy')\n",
    "\n",
    "    # Set the best gamma if it is a higher mean score  \n",
    "    if (np.mean(scores) > best_accuracy):\n",
    "        best_gamma = gamma\n",
    "        best_accuracy = np.mean(scores)\n",
    "\n",
    "print(best_gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2 Again, plot the ROC curve and report the confusion matrix and calculate the accuracy, recall precision and F-1 score of this best SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup SVM for best SVM \n",
    "best_svm = SVC(kernel='linear', C=best_gamma)\n",
    "best_svm = best_svm.fit(X_train_lsi, Y_train)\n",
    "Y_test_pred_best = best_svm.predict(X_test_lsi)\n",
    "\n",
    "print(\"Cross Validation\")\n",
    "\n",
    "# ROC Curve \n",
    "fit_predict_and_plot_roc(best_svm, X_train_lsi, Y_train, X_test_lsi, Y_test, \"Best SVM ROC Curve\")\n",
    "\n",
    "# Confusion Matrix \n",
    "print('Confusion:\\n', confusion_matrix(Y_test, Y_test_pred_best))\n",
    "best_cm = confusion_matrix(Y_test, Y_test_pred_best) \n",
    "plt.figure(); plot_confusion_matrix(best_cm, classes=class_names, title='Best SVM Confusion Matrix')\n",
    "\n",
    "# Statistics \n",
    "print('Accuracy:', accuracy_score(Y_test, Y_test_pred_best))\n",
    "print('Recall:', recall_score(Y_test, Y_test_pred_best))\n",
    "print('Precision:', precision_score(Y_test, Y_test_pred_best))\n",
    "print('F-1 Score:', f1_score(Y_test, Y_test_pred_best))\n",
    "print('F-1 Score:', f1_score(Y_test, Y_test_pred_best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6: Logistic Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.1 Train a logistic classifier without regularization (you may need to come up with some way to approximate this if you use sklearn.linear model.LogisticRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train logistic regression classifier without regularization\n",
    "lr_wo_reg = LogisticRegression(C=10**10, solver='liblinear', random_state=42)\n",
    "lr_wo_reg = lr_wo_reg.fit(X_train_lsi, Y_train)\n",
    "\n",
    "Y_test_pred_lr = lr_wo_reg.predict(X_test_lsi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.2 Plot the ROC curve and report the confusion matrix and calculate the accuracy, recall precision and F-1score of this classifier on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cross Validation\")\n",
    "\n",
    "# ROC Curve \n",
    "fit_predict_and_plot_roc(lr_wo_reg, X_train_lsi, Y_train, X_test_lsi, Y_test, \"Logistic Regression w/o Regularization ROC Curve \")\n",
    "\n",
    "# Confusion Matrix \n",
    "print('Confusion:\\n', confusion_matrix(Y_test, Y_test_pred_lr))\n",
    "lr_wo_reg_cm = confusion_matrix(Y_test, Y_test_pred_lr) \n",
    "plt.figure(); plot_confusion_matrix(lr_wo_reg_cm, classes=class_names, title='Logistic Regression without Regularization Confusion Matrix')\n",
    "\n",
    "# Statistics \n",
    "print('Accuracy:', accuracy_score(Y_test, Y_test_pred_lr))\n",
    "print('Recall:', recall_score(Y_test, Y_test_pred_lr))\n",
    "print('Precision:', precision_score(Y_test, Y_test_pred_lr))\n",
    "print('F-1 Score:', f1_score(Y_test, Y_test_pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Find the optimal regularization coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.1 Using 5-fold cross-validation on the dimension-reduced-by-SVD training data, find the optimal regularization strength in the range {10k|−5 ≤ k ≤ 5, k ∈ Z} for logistic regression with L1 regularization and logistic regression with L2 regularization, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding optimal regularization strength\n",
    "\n",
    "def opt_reg_strength(X_train, Y_train, penalty):\n",
    "    reg_strength = [10**k for k in range(-5, 6)]\n",
    "\n",
    "    best_strength = None\n",
    "    best_score = 0\n",
    "\n",
    "    # Creates a Logistic Regression for each strength level \n",
    "    for strength in reg_strength:\n",
    "        lr = LogisticRegression(C=strength, penalty=penalty, solver='liblinear', random_state=42).fit(X_train, Y_train)\n",
    "        scores = cross_val_score(lr, X_train, Y_train,cv=5, scoring='accuracy')\n",
    "\n",
    "        # updates best strength given if higher mean score \n",
    "        if (np.mean(scores) > best_score):\n",
    "            best_strength = strength\n",
    "            best_score = np.mean(scores)\n",
    "\n",
    "    return best_strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_strength_lr_l1 = opt_reg_strength(X_train_lsi, Y_train, 'l1')\n",
    "opt_strength_lr_l2 = opt_reg_strength(X_train_lsi, Y_train, 'l2')\n",
    "\n",
    "print(\"Optimal Regularization Strength for L1: \", opt_strength_lr_l1)\n",
    "print(\"Optimal Regularization Strength for L2: \", opt_strength_lr_l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.2 Compare the performance (accuracy, precision, recall and F-1 score) of 3 logistic classifiers: w/o regularization, w/ L1 regularization and w/ L2 regularization (with the best parameters you found from the part above), using test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance of Logistic Classifier w/o regularization\n",
    "lr_wo_reg = LogisticRegression(C=10**10, solver='liblinear', random_state=42)\n",
    "lr_wo_reg = lr_wo_reg.fit(X_train_lsi, Y_train)\n",
    "\n",
    "Y_test_pred_lr = lr_wo_reg.predict(X_test_lsi)\n",
    "\n",
    "# Statistics \n",
    "print(\"-\"*20 + \" Performance of Logistic Classifier w/o regularization \"+ \"-\"*20)\n",
    "print('Accuracy:', accuracy_score(Y_test, Y_test_pred_lr))\n",
    "print('Recall:', recall_score(Y_test, Y_test_pred_lr))\n",
    "print('Precision:', precision_score(Y_test, Y_test_pred_lr))\n",
    "print('F-1 Score:', f1_score(Y_test, Y_test_pred_lr))\n",
    "\n",
    "# Performance of Logistic Classifier w/ L1 regularization\n",
    "lr_w_l1 = LogisticRegression(C=opt_strength_lr_l1, solver='liblinear', penalty='l1', random_state=42)\n",
    "lr_w_l1 = lr_w_l1.fit(X_train_lsi, Y_train)\n",
    "\n",
    "Y_test_pred_lr_l1 = lr_w_l1.predict(X_test_lsi)\n",
    "\n",
    "# Statistics \n",
    "print(\"-\"*20 + \" Performance of Logistic Classifier w/ L1 regularization \"+ \"-\"*20)\n",
    "print('Accuracy:', accuracy_score(Y_test, Y_test_pred_lr_l1))\n",
    "print('Recall:', recall_score(Y_test, Y_test_pred_lr_l1))\n",
    "print('Precision:', precision_score(Y_test, Y_test_pred_lr_l1))\n",
    "print('F-1 Score:', f1_score(Y_test, Y_test_pred_lr_l1))\n",
    "\n",
    "# Performance of Logistic Classifier w/ L2 regularization\n",
    "lr_w_l2 = LogisticRegression(C=opt_strength_lr_l2, solver='liblinear', penalty='l2', random_state=42)\n",
    "lr_w_l2 = lr_w_l2.fit(X_train_lsi, Y_train)\n",
    "\n",
    "Y_test_pred_lr_l2 = lr_w_l2.predict(X_test_lsi)\n",
    "\n",
    "# Statistics \n",
    "print(\"-\"*20 + \" Performance of Logistic Classifier w/ L2 regularization \"+ \"-\"*20)\n",
    "print('Accuracy:', accuracy_score(Y_test, Y_test_pred_lr_l2))\n",
    "print('Recall:', recall_score(Y_test, Y_test_pred_lr_l2))\n",
    "print('Precision:', precision_score(Y_test, Y_test_pred_lr_l2))\n",
    "print('F-1 Score:', f1_score(Y_test, Y_test_pred_lr_l2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.3 How does the regularization parameter affect the test error? How are the learnt coefficients affected? Why might one be interested in each type of regularization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "incomplete (explain why)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.4 Both logistic regression and linear SVM are trying to classify data points using a linear decision boundary. What is the difference between their ways to find this boundary? Why do their performances differ? Is this difference statistically significant?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7: Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.1 Evaluate and profile a Naive Bayes classifier: Train a GaussianNB classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Train Gaussian Naive Bayes classifier \n",
    "gnb_cf = GaussianNB()\n",
    "\n",
    "gnb_cf = gnb_cf.fit(X_train_lsi, Y_train)\n",
    "\n",
    "Y_test_pred_gnb = gnb_cf.predict(X_test_lsi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.2 Plot the ROC curve and report the confusion matrix and calculate the accuracy, recall, precision and F-1 score of this classifier on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Naive Bayes Classifier\")\n",
    "\n",
    "# ROC Curve \n",
    "fit_predict_and_plot_roc(gnb_cf, X_train_lsi, Y_train, X_test_lsi, Y_test, \"Gaussian Naive Bayes Classifier ROC Curve\")\n",
    "\n",
    "# Confusion Matrix \n",
    "print('Confusion:\\n', confusion_matrix(Y_test, Y_test_pred_gnb))\n",
    "gnb_cm = confusion_matrix(Y_test, Y_test_pred_gnb) \n",
    "plt.figure(); plot_confusion_matrix(gnb_cm, classes=class_names, title='Gaussian Naive Bayes Classifier Confusion Matrix')\n",
    "\n",
    "# Statistics  \n",
    "print('Accuracy:', accuracy_score(Y_test, Y_test_pred_gnb))\n",
    "print('Recall:', recall_score(Y_test, Y_test_pred_gnb))\n",
    "print('Precision:', precision_score(Y_test, Y_test_pred_gnb))\n",
    "print('F-1 Score:', f1_score(Y_test, Y_test_pred_gnb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8: Binary Classification (Pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Construct a Pipeline that performs feature extraction, dimensionality reduction and classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(stop_words='english')), # vectorize (feature extraction)\n",
    "    ('tfidf', TfidfTransformer()), # transformer\n",
    "    ('reduce_dim', TruncatedSVD()), # dimensionality reduction\n",
    "    ('clf', SVC()), # classification\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 The evaluation of each combination is performed with 5-fold cross-validation (use the average validation set accuracy across folds)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2.1 In addition to any other hyperparameters you choose, your gridsearch must at least include: Loading Data, Feature Extraction, Dimensionality Reduction, Classifier, Other options. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check loading data section !!!!!!!!!\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# hyperparameters = [\n",
    "#     {\n",
    "#         'vect__min_df': [3,5], # Feature Extraction: min_df (3 vs 5),\n",
    "#         'reduce_dim': [# Dimensionality Reduction: LSI vs NMF (k = [5, 30, 80])\n",
    "#             TruncatedSVD(n_components=5, random_state=42),\n",
    "#             TruncatedSVD(n_components=30, random_state=42),\n",
    "#             TruncatedSVD(n_components=80, random_state=42),\n",
    "#             NMF(n_components=5, random_state=42),\n",
    "#             NMF(n_components=30, random_state=42),\n",
    "#             NMF(n_components=80, random_state=42)\n",
    "#         ], \n",
    "#         'clf': [ # Classifier: SVM (best gamma), L1 Regularization, L2 Regularization, GaussianNB\n",
    "#             SVC(C=best_gamma, kernel='linear'), \n",
    "#             LogisticRegression(C=opt_strength_lr_l1, penalty = 'l1', solver='liblinear', random_state=42),\n",
    "#             LogisticRegression(C=opt_strength_lr_l2, penalty = 'l2', random_state=42),\n",
    "#             GaussianNB(),\n",
    "#         ]\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "# For testing purposes \n",
    "hyperparameters = [\n",
    "    {\n",
    "        'vect__min_df': [3,5], # Feature Extraction: min_df (3 vs 5)\n",
    "        'reduce_dim': [# Dimensionality Reduction: LSI vs NMF (k = [5, 30, 80])\n",
    "            TruncatedSVD(n_components=5, random_state=42),\n",
    "        ], \n",
    "        'clf': [ # Classifier: SVM (best gamma), L1 Regularization, L2 Regularization, GaussianNB\n",
    "            SVC(C=best_gamma, kernel='linear'), \n",
    "        ]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create Grid \n",
    "\n",
    "grid_lemmatized = GridSearchCV(\n",
    "    pipeline, \n",
    "    cv=5, \n",
    "    n_jobs=-1, \n",
    "    param_grid=hyperparameters, \n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "grid_stemmed = GridSearchCV(\n",
    "    pipeline, \n",
    "    cv=5, \n",
    "    n_jobs=-1, \n",
    "    param_grid=hyperparameters, \n",
    "    scoring='accuracy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatized Fit and Predict \n",
    "grid_lemmatized = grid_lemmatized.fit(train_lemmatized, Y_train)\n",
    "grid_lemmatized_pred = grid_lemmatized.predict(test_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stem\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "\n",
    "# nltk.download()\n",
    "\n",
    "# Stem entire data \n",
    "def stem(data):\n",
    "    stemmed_data = [] \n",
    "    for text in data:\n",
    "        # print(\"text\": text)\n",
    "        # Return a tokenized copy of text \n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        # print(tokens)\n",
    "        \n",
    "        # tags given list of tokens \n",
    "        tagged = pos_tag(tokens)\n",
    "        # print(tagged)\n",
    "\n",
    "        # lemmatize text excluding numbers \n",
    "        stemmed_words = []\n",
    "        for word in tokens:\n",
    "            # print(\"word: \" word)\n",
    "            if not re.match(r'^\\d+(\\.\\d+)?$', word): # double check this\n",
    "                stemmed_word = PorterStemmer().stem(word.lower())\n",
    "                stemmed_words.append(stemmed_word)\n",
    "            \n",
    "        stemmed_sentence = ' '.join(stemmed_words)\n",
    "        \n",
    "        # Add whole sentence back to the lemmatize_data array \n",
    "        stemmed_data.append(stemmed_sentence)\n",
    "\n",
    "    return stemmed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemmed data \n",
    "train_stemmed = stem(train['full_text'])\n",
    "test_stemmed = stem(test['full_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemmed Fit and Predict \n",
    "grid_stemmed = grid_stemmed.fit(train_stemmed, Y_train)\n",
    "grid_stemmed_pred = grid_stemmed.predict(test_stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 What are the 5 best combinations? Report their performances on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Model and Best Parameters \n",
    "# grid_model_best_lemmatized = grid_lemmatized.best_estimator_\n",
    "# grid_params_best_lemmatized = grid_lemmatized.best_params_\n",
    "grid_lemmatized_result = pd.DataFrame(grid_lemmatized.cv_results_)\n",
    "\n",
    "# grid_model_best_stemmed = grid_stemmed.best_estimator_\n",
    "# grid_params_best_stemmed = grid_stemmed.best_params_\n",
    "grid_stemmed_result = pd.DataFrame(grid_stemmed.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Best Results \n",
    "grid_lemmatized_result['lemmatized'] = True\n",
    "grid_stemmed_result['lemmatized'] = False\n",
    "results = pd.concat([grid_lemmatized_result, grid_lemmatized_result])\n",
    "results = results[['mean_test_score', 'param_clf', 'param_reduce_dim','param_vect__min_df', 'lemmatized']].sort_values(by=['mean_test_score'], ascending=False)\n",
    "results = results.reset_index(drop=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.1 Perform Naive Bayes classification and multiclass SVM classification (with both One VS One and One VS the rest methods described above). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiclass Train and Test \n",
    "mc_train, mc_test = train_test_split(df[[\"full_text\",\"leaf_label\"]], test_size=0.2)\n",
    "\n",
    "mc_train['full_text'] = mc_train['full_text'].map(clean)\n",
    "mc_test['full_text'] = mc_test['full_text'].map(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Mapping \n",
    "\n",
    "map_row_to_class = {0:\"basketball\", 1:\"baseball\", 2:\"tennis\",\n",
    "3:\"football\", 4:\"soccer\", 5:\"forest fire\", 6:\"flood\",\n",
    "7:\"earthquake\", 8:\"drought\", 9:\"heatwave\"}\n",
    "\n",
    "reverse_row_to_class = {\n",
    "    v: k for k, v in map_row_to_class.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "mc_train_lemmatized = lemmatize(mc_train['full_text'])\n",
    "mc_test_lemmatized = lemmatize(mc_test['full_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Vectorizer\n",
    "mc_count_vect = CountVectorizer(min_df=3, stop_words=\"english\")\n",
    "mc_X_train_counts = mc_count_vect.fit_transform(mc_train_lemmatized).toarray()\n",
    "mc_X_test_counts = mc_count_vect.transform(mc_test_lemmatized).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF\n",
    "mc_tfidf_transformer = TfidfTransformer()\n",
    "mc_X_train_tfidf = mc_tfidf_transformer.fit_transform(mc_X_train_counts).toarray()\n",
    "mc_X_test_tfidf = mc_tfidf_transformer.transform(mc_X_test_counts).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSI\n",
    "mc_lsi = TruncatedSVD(n_components=50, random_state=42)\n",
    "mc_X_train_lsi = mc_lsi.fit_transform(mc_X_train_tfidf)\n",
    "mc_X_test_lsi = mc_lsi.transform(mc_X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NMF\n",
    "mc_nmf = NMF(n_components=50, max_iter=2000, random_state=42)\n",
    "mc_X_train_NMF = mc_nmf.fit_transform(mc_X_train_tfidf)\n",
    "mc_X_test_NMF = mc_nmf.transform(mc_X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up Y train and test based on mapping \n",
    "mc_train['mc_label'] = mc_train['leaf_label'].apply(lambda x: reverse_row_to_class[x] if x in reverse_row_to_class else 'dumb')\n",
    "mc_test['mc_label'] = mc_test['leaf_label'].apply(lambda x: reverse_row_to_class[x] if x in reverse_row_to_class else 'dumb')\n",
    "mc_Y_train = mc_train['mc_label']\n",
    "mc_Y_test = mc_test['mc_label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.2 Report the confusion matrix and calculate the accuracy, recall, precision and F-1 score of your classifiers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make report generic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_predict(cf, X_train, Y_train, X_test): \n",
    "    cf = cf.fit(X_train, Y_train)\n",
    "    Y_pred = cf.predict(X_test)\n",
    "    return Y_pred\n",
    "\n",
    "def cf_plot(Y_test, Y_pred, classifier_name): \n",
    "    # Confusion Matrix \n",
    "    cm = confusion_matrix(Y_test, Y_pred) \n",
    "    print('Confusion:\\n', cm)\n",
    "    plt.figure(); plot_confusion_matrix(cm, classes=map_row_to_class, title='Multiclass {classifier_name} Classifier Confusion Matrix')\n",
    "\n",
    "def statistics(Y_test, Y_pred):\n",
    "    # Statistics  \n",
    "    print('Accuracy:', accuracy_score(Y_test, Y_pred))\n",
    "    print('Recall:', recall_score(Y_test, Y_pred, average='macro'))\n",
    "    print('Precision:', precision_score(Y_test, Y_pred, average='macro'))\n",
    "    print('F-1 Score:', f1_score(Y_test, Y_pred, average='macro')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_report(cf, X_train, X_test, Y_train, Y_test): \n",
    "    Y_pred = fit_and_predict(cf, X_train, Y_train, X_test)\n",
    "    cf_plot(Y_test, Y_pred)\n",
    "    statistics(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through Count Vectorizor, TFIDF, LSI, and NMF \n",
    "mc_classifiers = {\n",
    "    \"Count Vectorizor\": [mc_X_train_counts, mc_X_test_counts], \n",
    "    # \"TF-IDF\": [mc_X_train_tfidf, mc_X_test_tfidf], \n",
    "    # \"LSI\": [mc_X_train_lsi, mc_X_test_lsi], \n",
    "    # \"NMF\": [mc_X_train_NMF, mc_X_test_NMF], \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear SVC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear SVC \n",
    "for classifier, (X_train, X_test) in mc_classifiers.items():\n",
    "    print(classifier)\n",
    "    mc_cf = SVC(C=100000, kernel='linear', random_state=42)\n",
    "    multiclass_report(mc_cf, X_train, X_test, mc_Y_train, mc_Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes \n",
    "for classifier, (X_train, X_test) in mc_classifiers.items():\n",
    "    mc_cf = GaussianNB()\n",
    "    multiclass_report(mc_cf, X_train, X_test, mc_Y_train, mc_Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One VS One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One VS One \n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "\n",
    "for classifier, (X_train, X_test) in mc_classifiers.items():\n",
    "    mc_cf = OneVsOneClassifier(SVC(kernel='linear', random_state=42))\n",
    "    multiclass_report(mc_cf, X_train, X_test, mc_Y_train, mc_Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One VS Rest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One VS Rest \n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "for classifier, (X_train, X_test) in mc_classifiers.items():\n",
    "    mc_cf = OneVsRestClassifier(SVC(kernel='linear', random_state=42))\n",
    "    multiclass_report(mc_cf, X_train, X_test, mc_Y_train, mc_Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear SVC + Count Vectorizer \n",
    "mc_gnb_cf = SVC(C=100000, kernel='linear', random_state=42)\n",
    "mc_gnb_cf = mc_gnb_cf.fit(mc_X_train_counts, mc_Y_train)\n",
    "mc_Y_test_pred_gnb = mc_gnb_cf.predict(mc_X_test_counts)\n",
    "\n",
    "# Confusion Matrix \n",
    "print('Confusion:\\n', confusion_matrix(mc_Y_test, mc_Y_test_pred_gnb))\n",
    "mc_gnb_cm = confusion_matrix(mc_Y_test, mc_Y_test_pred_gnb) \n",
    "plt.figure(); plot_confusion_matrix(mc_gnb_cm, classes=map_row_to_class, title='Multiclass Linear SVC + Count Vectorizor Classifier Confusion Matrix')\n",
    "\n",
    "# Statistics  \n",
    "print('Accuracy:', accuracy_score(mc_Y_test, mc_Y_test_pred_gnb))\n",
    "print('Recall:', recall_score(mc_Y_test, mc_Y_test_pred_gnb, average='macro'))\n",
    "print('Precision:', precision_score(mc_Y_test, mc_Y_test_pred_gnb, average='macro'))\n",
    "print('F-1 Score:', f1_score(mc_Y_test, mc_Y_test_pred_gnb, average='macro')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear SVC + TFIDF \n",
    "mc_gnb_cf = SVC(C=100000, kernel='linear', random_state=42)\n",
    "mc_gnb_cf = mc_gnb_cf.fit(mc_X_train_tfidf, mc_Y_train)\n",
    "mc_Y_test_pred_gnb = mc_gnb_cf.predict(mc_X_test_tfidf)\n",
    "\n",
    "# Confusion Matrix \n",
    "print('Confusion:\\n', confusion_matrix(mc_Y_test, mc_Y_test_pred_gnb))\n",
    "mc_gnb_cm = confusion_matrix(mc_Y_test, mc_Y_test_pred_gnb) \n",
    "plt.figure(); plot_confusion_matrix(mc_gnb_cm, classes=map_row_to_class, title='Multiclass Linear SVC + TF-IDF Classifier Confusion Matrix')\n",
    "\n",
    "# Statistics  \n",
    "print('Accuracy:', accuracy_score(mc_Y_test, mc_Y_test_pred_gnb))\n",
    "print('Recall:', recall_score(mc_Y_test, mc_Y_test_pred_gnb, average='macro'))\n",
    "print('Precision:', precision_score(mc_Y_test, mc_Y_test_pred_gnb, average='macro'))\n",
    "print('F-1 Score:', f1_score(mc_Y_test, mc_Y_test_pred_gnb, average='macro')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear SVC + LSI \n",
    "mc_gnb_cf = SVC(C=100000, kernel='linear', random_state=42)\n",
    "mc_gnb_cf = mc_gnb_cf.fit(mc_X_train_lsi, mc_Y_train)\n",
    "mc_Y_test_pred_gnb = mc_gnb_cf.predict(mc_X_test_lsi)\n",
    "\n",
    "# Confusion Matrix \n",
    "print('Confusion:\\n', confusion_matrix(mc_Y_test, mc_Y_test_pred_gnb))\n",
    "mc_gnb_cm = confusion_matrix(mc_Y_test, mc_Y_test_pred_gnb) \n",
    "plt.figure(); plot_confusion_matrix(mc_gnb_cm, classes=map_row_to_class, title='Multiclass Linear SVC + LSI Classifier Confusion Matrix')\n",
    "\n",
    "# Statistics  \n",
    "print('Accuracy:', accuracy_score(mc_Y_test, mc_Y_test_pred_gnb))\n",
    "print('Recall:', recall_score(mc_Y_test, mc_Y_test_pred_gnb, average='macro'))\n",
    "print('Precision:', precision_score(mc_Y_test, mc_Y_test_pred_gnb, average='macro'))\n",
    "print('F-1 Score:', f1_score(mc_Y_test, mc_Y_test_pred_gnb, average='macro')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear SVC + NMF \n",
    "mc_gnb_cf = SVC(C=100000, kernel='linear', random_state=42)\n",
    "mc_gnb_cf = mc_gnb_cf.fit(mc_X_train_NMF, mc_Y_train)\n",
    "mc_Y_test_pred_gnb = mc_gnb_cf.predict(mc_X_test_NMF)\n",
    "\n",
    "# Confusion Matrix \n",
    "print('Confusion:\\n', confusion_matrix(mc_Y_test, mc_Y_test_pred_gnb))\n",
    "mc_gnb_cm = confusion_matrix(mc_Y_test, mc_Y_test_pred_gnb) \n",
    "plt.figure(); plot_confusion_matrix(mc_gnb_cm, classes=map_row_to_class, title='Multiclass Linear SVC + NMF  Classifier Confusion Matrix')\n",
    "\n",
    "# Statistics  \n",
    "print('Accuracy:', accuracy_score(mc_Y_test, mc_Y_test_pred_gnb))\n",
    "print('Recall:', recall_score(mc_Y_test, mc_Y_test_pred_gnb, average='macro'))\n",
    "print('Precision:', precision_score(mc_Y_test, mc_Y_test_pred_gnb, average='macro'))\n",
    "print('F-1 Score:', f1_score(mc_Y_test, mc_Y_test_pred_gnb, average='macro')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes + CountVectorizer \n",
    "mc_gnb_cf = GaussianNB()\n",
    "mc_gnb_cf = mc_gnb_cf.fit(mc_X_train_counts, mc_Y_train)\n",
    "mc_Y_test_pred_gnb = mc_gnb_cf.predict(mc_X_test_counts)\n",
    "\n",
    "# Confusion Matrix \n",
    "print('Confusion:\\n', confusion_matrix(mc_Y_test, mc_Y_test_pred_gnb))\n",
    "mc_gnb_cm = confusion_matrix(mc_Y_test, mc_Y_test_pred_gnb) \n",
    "plt.figure(); plot_confusion_matrix(mc_gnb_cm, classes=map_row_to_class, title='Multiclass Gaussian Naive Bayes + Count Vectorizer Classifier Confusion Matrix')\n",
    "\n",
    "# Statistics  \n",
    "print('Accuracy:', accuracy_score(mc_Y_test, mc_Y_test_pred_gnb))\n",
    "print('Recall:', recall_score(mc_Y_test, mc_Y_test_pred_gnb, average='macro'))\n",
    "print('Precision:', precision_score(mc_Y_test, mc_Y_test_pred_gnb, average='macro'))\n",
    "print('F-1 Score:', f1_score(mc_Y_test, mc_Y_test_pred_gnb, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes + TFIDF \n",
    "mc_gnb_cf = GaussianNB()\n",
    "mc_gnb_cf = mc_gnb_cf.fit(mc_X_train_tfidf, mc_Y_train)\n",
    "mc_Y_test_pred_gnb = mc_gnb_cf.predict(mc_X_test_tfidf)\n",
    "\n",
    "# Confusion Matrix \n",
    "print('Confusion:\\n', confusion_matrix(mc_Y_test, mc_Y_test_pred_gnb))\n",
    "mc_gnb_cm = confusion_matrix(mc_Y_test, mc_Y_test_pred_gnb) \n",
    "plt.figure(); plot_confusion_matrix(mc_gnb_cm, classes=map_row_to_class, title='Multiclass Gaussian Naive Bayes + TF-IDF Classifier Confusion Matrix')\n",
    "\n",
    "# Statistics  \n",
    "print('Accuracy:', accuracy_score(mc_Y_test, mc_Y_test_pred_gnb))\n",
    "print('Recall:', recall_score(mc_Y_test, mc_Y_test_pred_gnb, average='macro'))\n",
    "print('Precision:', precision_score(mc_Y_test, mc_Y_test_pred_gnb, average='macro'))\n",
    "print('F-1 Score:', f1_score(mc_Y_test, mc_Y_test_pred_gnb, average='macro')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes + LSI \n",
    "\n",
    "mc_gnb_cf = GaussianNB()\n",
    "mc_gnb_cf = mc_gnb_cf.fit(mc_X_train_lsi, mc_Y_train)\n",
    "mc_Y_test_pred_gnb = mc_gnb_cf.predict(mc_X_test_lsi)\n",
    "\n",
    "# Confusion Matrix \n",
    "print('Confusion:\\n', confusion_matrix(mc_Y_test, mc_Y_test_pred_gnb))\n",
    "mc_gnb_cm = confusion_matrix(mc_Y_test, mc_Y_test_pred_gnb) \n",
    "plt.figure(); plot_confusion_matrix(mc_gnb_cm, classes=map_row_to_class, title='Multiclass Gaussian Naive Bayes + LSI Classifier Confusion Matrix')\n",
    "\n",
    "# Statistics  \n",
    "print('Accuracy:', accuracy_score(mc_Y_test, mc_Y_test_pred_gnb))\n",
    "print('Recall:', recall_score(mc_Y_test, mc_Y_test_pred_gnb, average='macro'))\n",
    "print('Precision:', precision_score(mc_Y_test, mc_Y_test_pred_gnb, average='macro'))\n",
    "print('F-1 Score:', f1_score(mc_Y_test, mc_Y_test_pred_gnb, average='macro')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes + NMF \n",
    "\n",
    "mc_gnb_cf = GaussianNB()\n",
    "# mc_X_train_counts = mc_X_train_counts.toarray()\n",
    "mc_gnb_cf = mc_gnb_cf.fit(mc_X_train_NMF, mc_Y_train)\n",
    "# mc_X_test_counts = mc_X_test_counts.toarray()\n",
    "mc_Y_test_pred_gnb = mc_gnb_cf.predict(mc_X_test_NMF)\n",
    "\n",
    "# mc_X_test_NMFx \n",
    "print('Confusion:\\n', confusion_matrix(mc_Y_test, mc_Y_test_pred_gnb))\n",
    "mc_gnb_cm = confusion_matrix(mc_Y_test, mc_Y_test_pred_gnb) \n",
    "plt.figure(); plot_confusion_matrix(mc_gnb_cm, classes=map_row_to_class, title='Multiclass Gaussian Naive Bayes + NMF Classifier Confusion Matrix')\n",
    "\n",
    "# Statistics  \n",
    "print('Accuracy:', accuracy_score(mc_Y_test, mc_Y_test_pred_gnb))\n",
    "print('Recall:', recall_score(mc_Y_test, mc_Y_test_pred_gnb, average='macro'))\n",
    "print('Precision:', precision_score(mc_Y_test, mc_Y_test_pred_gnb, average='macro'))\n",
    "print('F-1 Score:', f1_score(mc_Y_test, mc_Y_test_pred_gnb, average='macro')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One VS One Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# One VS One + Count Vectorizor  \n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "\n",
    "mc_1v1 = OneVsOneClassifier(SVC(kernel='linear', random_state=42))\n",
    "mc_1v1.fit(mc_X_train_counts, mc_Y_train)\n",
    "mc_Y_test_pred_1v1 = mc_1v1.predict(mc_X_test_counts)\n",
    "\n",
    "# Confusion Matrix \n",
    "print('Confusion:\\n', confusion_matrix(mc_Y_test, mc_Y_test_pred_1v1))\n",
    "mc_gnb_cm = confusion_matrix(mc_Y_test, mc_Y_test_pred_1v1) \n",
    "plt.figure(); plot_confusion_matrix(mc_1v1, classes=map_row_to_class, title='Multiclass One VS One + Count Vectorizor Confusion Matrix')\n",
    "\n",
    "# Statistics  \n",
    "print('Accuracy:', accuracy_score(mc_Y_test, mc_Y_test_pred_1v1))\n",
    "print('Recall:', recall_score(mc_Y_test, mc_Y_test_pred_1v1, average='macro'))\n",
    "print('Precision:', precision_score(mc_Y_test, mc_Y_test_pred_1v1, average='macro'))\n",
    "print('F-1 Score:', f1_score(mc_Y_test, mc_Y_test_pred_1v1, average='macro')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One VS One + TFIDF \n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "\n",
    "mc_1v1 = OneVsOneClassifier(SVC(kernel='linear', random_state=42))\n",
    "mc_1v1.fit(mc_X_train_tfidf, mc_Y_train)\n",
    "mc_Y_test_pred_1v1 = mc_1v1.predict(mc_X_test_tfidf)\n",
    "\n",
    "# Confusion Matrix \n",
    "print('Confusion:\\n', confusion_matrix(mc_Y_test, mc_Y_test_pred_1v1))\n",
    "mc_gnb_cm = confusion_matrix(mc_Y_test, mc_Y_test_pred_1v1) \n",
    "plt.figure(); plot_confusion_matrix(mc_1v1, classes=map_row_to_class, title='Multiclass One VS One + TFIDF Confusion Matrix')\n",
    "\n",
    "# Statistics  \n",
    "print('Accuracy:', accuracy_score(mc_Y_test, mc_Y_test_pred_1v1))\n",
    "print('Recall:', recall_score(mc_Y_test, mc_Y_test_pred_1v1, average='macro'))\n",
    "print('Precision:', precision_score(mc_Y_test, mc_Y_test_pred_1v1, average='macro'))\n",
    "print('F-1 Score:', f1_score(mc_Y_test, mc_Y_test_pred_1v1, average='macro')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One VS One + LSI\n",
    "mc_1v1 = OneVsOneClassifier(SVC(kernel='linear', random_state=42))\n",
    "mc_1v1.fit(mc_X_train_lsi, mc_Y_train)\n",
    "mc_Y_test_pred_1v1 = mc_1v1.predict(mc_X_test_lsi)\n",
    "\n",
    "# Confusion Matrix \n",
    "print('Confusion:\\n', confusion_matrix(mc_Y_test, mc_Y_test_pred_1v1))\n",
    "mc_gnb_cm = confusion_matrix(mc_Y_test, mc_Y_test_pred_1v1) \n",
    "plt.figure(); plot_confusion_matrix(mc_1v1, classes=map_row_to_class, title='Multiclass One VS One + LSI Confusion Matrix')\n",
    "\n",
    "# Statistics  \n",
    "print('Accuracy:', accuracy_score(mc_Y_test, mc_Y_test_pred_1v1))\n",
    "print('Recall:', recall_score(mc_Y_test, mc_Y_test_pred_1v1, average='macro'))\n",
    "print('Precision:', precision_score(mc_Y_test, mc_Y_test_pred_1v1, average='macro'))\n",
    "print('F-1 Score:', f1_score(mc_Y_test, mc_Y_test_pred_1v1, average='macro')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One VS One + NMF\n",
    "mc_1v1 = OneVsOneClassifier(SVC(kernel='linear', random_state=42))\n",
    "mc_1v1.fit(mc_X_train_NMF, mc_Y_train)\n",
    "mc_Y_test_pred_1v1 = mc_1v1.predict(mc_X_test_NMF)\n",
    "\n",
    "# Confusion Matrix \n",
    "print('Confusion:\\n', confusion_matrix(mc_Y_test, mc_Y_test_pred_1v1))\n",
    "mc_gnb_cm = confusion_matrix(mc_Y_test, mc_Y_test_pred_1v1) \n",
    "plt.figure(); plot_confusion_matrix(mc_1v1, classes=map_row_to_class, title='Multiclass One VS One + NMF Confusion Matrix')\n",
    "\n",
    "# Statistics  \n",
    "print('Accuracy:', accuracy_score(mc_Y_test, mc_Y_test_pred_1v1))\n",
    "print('Recall:', recall_score(mc_Y_test, mc_Y_test_pred_1v1, average='macro'))\n",
    "print('Precision:', precision_score(mc_Y_test, mc_Y_test_pred_1v1, average='macro'))\n",
    "print('F-1 Score:', f1_score(mc_Y_test, mc_Y_test_pred_1v1, average='macro')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One VS Rest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One VS Rest + Count Vectorizor \n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "mc_1vR = OneVsRestClassifier(SVC(kernel='linear', random_state=42))\n",
    "mc_1v1.fit(mc_X_train_counts, mc_Y_train)\n",
    "mc_Y_test_pred_1v1 = mc_1v1.predict(mc_X_test_counts)\n",
    "\n",
    "# Confusion Matrix \n",
    "print('Confusion:\\n', confusion_matrix(mc_Y_test, mc_Y_test_pred_1v1))\n",
    "mc_gnb_cm = confusion_matrix(mc_Y_test, mc_Y_test_pred_1v1) \n",
    "plt.figure(); plot_confusion_matrix(mc_1v1, classes=map_row_to_class, title='Multiclass One VS One + Count Vectorizor Confusion Matrix')\n",
    "\n",
    "# Statistics  \n",
    "print('Accuracy:', accuracy_score(mc_Y_test, mc_Y_test_pred_1v1))\n",
    "print('Recall:', recall_score(mc_Y_test, mc_Y_test_pred_1v1, average='macro'))\n",
    "print('Precision:', precision_score(mc_Y_test, mc_Y_test_pred_1v1, average='macro'))\n",
    "print('F-1 Score:', f1_score(mc_Y_test, mc_Y_test_pred_1v1, average='macro')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One VS Rest + TFIDF\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "mc_1vR = OneVsRestClassifier(SVC(kernel='linear', random_state=42))\n",
    "mc_1vR.fit(mc_X_test_tfidf, mc_Y_train)\n",
    "mc_Y_test_pred_1vR = mc_1vR.predict(mc_X_test_tfidf)\n",
    "\n",
    "# Confusion Matrix \n",
    "print('Confusion:\\n', confusion_matrix(mc_Y_test, mc_Y_test_pred_1vR))\n",
    "mc_gnb_cm = confusion_matrix(mc_Y_test, mc_Y_test_pred_1vR) \n",
    "plt.figure(); plot_confusion_matrix(mc_1vR, classes=map_row_to_class, title='Multiclass One VS Rest + TFIDF Confusion Matrix')\n",
    "\n",
    "# Statistics  \n",
    "print('Accuracy:', accuracy_score(mc_Y_test, mc_Y_test_pred_1vR))\n",
    "print('Recall:', recall_score(mc_Y_test, mc_Y_test_pred_1vR, average='macro'))\n",
    "print('Precision:', precision_score(mc_Y_test, mc_Y_test_pred_1vR, average='macro'))\n",
    "print('F-1 Score:', f1_score(mc_Y_test, mc_Y_test_pred_1vR, average='macro')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One VS Rest + LSI\n",
    "mc_1vR = OneVsRestClassifier(SVC(kernel='linear', random_state=42))\n",
    "mc_1vR.fit(mc_X_test_lsi, mc_Y_train)\n",
    "mc_Y_test_pred_1vR = mc_1vR.predict(mc_X_test_lsi)\n",
    "\n",
    "# Confusion Matrix \n",
    "print('Confusion:\\n', confusion_matrix(mc_Y_test, mc_Y_test_pred_1vR))\n",
    "mc_gnb_cm = confusion_matrix(mc_Y_test, mc_Y_test_pred_1vR) \n",
    "plt.figure(); plot_confusion_matrix(mc_1vR, classes=map_row_to_class, title='Multiclass One VS Rest + LSI Confusion Matrix')\n",
    "\n",
    "# Statistics  \n",
    "print('Accuracy:', accuracy_score(mc_Y_test, mc_Y_test_pred_1vR))\n",
    "print('Recall:', recall_score(mc_Y_test, mc_Y_test_pred_1vR, average='macro'))\n",
    "print('Precision:', precision_score(mc_Y_test, mc_Y_test_pred_1vR, average='macro'))\n",
    "print('F-1 Score:', f1_score(mc_Y_test, mc_Y_test_pred_1vR, average='macro')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One VS Rest + NMF\n",
    "mc_1vR = OneVsRestClassifier(SVC(kernel='linear', random_state=42))\n",
    "mc_1vR.fit(mc_X_test_NMF, mc_Y_train)\n",
    "mc_Y_test_pred_1vR = mc_1vR.predict(mc_X_test_NMF)\n",
    "\n",
    "# Confusion Matrix \n",
    "print('Confusion:\\n', confusion_matrix(mc_Y_test, mc_Y_test_pred_1vR))\n",
    "mc_gnb_cm = confusion_matrix(mc_Y_test, mc_Y_test_pred_1vR) \n",
    "plt.figure(); plot_confusion_matrix(mc_1vR, classes=map_row_to_class, title='Multiclass One VS Rest + NMF Confusion Matrix')\n",
    "\n",
    "# Statistics  \n",
    "print('Accuracy:', accuracy_score(mc_Y_test, mc_Y_test_pred_1vR))\n",
    "print('Recall:', recall_score(mc_Y_test, mc_Y_test_pred_1vR, average='macro'))\n",
    "print('Precision:', precision_score(mc_Y_test, mc_Y_test_pred_1vR, average='macro'))\n",
    "print('F-1 Score:', f1_score(mc_Y_test, mc_Y_test_pred_1vR, average='macro')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.3 How did you resolve the class imbalance issue in the One VS the rest model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2.1 In the confusion matrix you should have an 10 × 10 matrix where 10 is the number of unique labels in the column leaf label. Please make sure that the order of these labels is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2.2 Do you observe any structure in the confusion matrix? Are there distinct visible blocks on the major diagonal? What does this mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Based on your observation from the previous part, suggest a subset of labels that should be merged into a new larger label and recompute the accuracy and plot the confusion matrix. How did the accuracy change in One VS One and One VS the rest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4 Does class imbalance impact the performance of the classification once some classes are merged? Provide a resolution for the class imbalance and recompute the accuracy and plot the confusion matrix in One VS One and One VS the rest?."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10 (a): \n",
    "- Answer a) GLoVE embeddings are trained on the ratio of co-occurance of probabilities as they aim to capture each word as uniquely as possible. The ratio also helps in removing the influence of some common words like 'a' and 'is' which helps the glove embeddings to capture lesser seen relationships that are more informative in real world.\n",
    "\n",
    "### Question 10 (b): \n",
    "- Answer b) GLoVE embeddings would return the same vector in both cases as the GLoVE embedding vectors are pre-trained static vectors. These pre-trained vectors are obtained after training on a large dataset so the capture all the information related to one word in the static vectors that we use. \n",
    "\n",
    "### Question 10 (c): \n",
    "- Answer c) $||GLoVE['woman'] - GLoVE['man']||_2$ $||GLoVE['wife'] - GLoVE['husband']||_2$ $||GLoVE['wife'] - GLoVE['orange']||_2$\n",
    "\n",
    "The given l2 norm between the GLoVE embedding vectors of \"woman\" and \"man\" is expected to be small as the probability that the two words \"woman\" and \"man\" occuring together is higher than the probability of words \"wife\" and \"orange\". Whereas, the l2 norm of \"wife\" and \"husband\" and \"woman\" and \"man\" will not be very differnt.\n",
    "\n",
    "Conclusion: The l2 norm of \"woman\" and \"man\" will be close to the l2 norm of \"wife\" and \"husband\" and will be a smaller value. Whereas, the l2 norm of \"wife\" and \"orange\" will be a larger value and this value will be vary a lot as compared to the other values. \n",
    "\n",
    "### Question 10 (d):\n",
    "- Answer d) We would lemmatize the word before mapping it to its GLoVE embedding and not stem it as, lemmatization keeps the meaning of the word intact by just recuding the word to it's corresponding stem value. Wheareas, stemming does not consider the meaning it just removes the prefix or suffix attached to the word, the resulting word after stemming need not necessarily make sense or have a meaning.\n",
    "As the GLoVe embeddings are vectors available for actual words we should use lemmatization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 11\n",
    "For the binary classification task distinguishing the “sports” class and “climate” class. Describe a feature engineering process that uses GLoVE word embeddings to represent each document.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer: \n",
    "The feature engineering process that we used is as follows:\n",
    "1) We use the keywords column given in the data. \n",
    "2) We then find the glove embedding for each keyword of a data point. \n",
    "3) We normalize and get a vector representation of 300 for each data point. We will then use this representation to perform classification. \n",
    "\n",
    "Got an accuracy of 0.9410919540229885"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to load the GloVe Embeddings\n",
    "def load_glove_embeddings(file):\n",
    "    embeddings_dict = {}\n",
    "    dimension_of_glove = 300\n",
    "    with open(file, 'r', encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], \"float32\")\n",
    "            embeddings_dict[word] = vector\n",
    "#             print(f\"embeddings_dict[word], word:{embeddings_dict[word], word}\")\n",
    "    return embeddings_dict\n",
    "\n",
    "# word_embeddings is a dictionary that contains words as keys\n",
    "word_embeddings = load_glove_embeddings(\"glove.6B.300d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_for_example = word_embeddings['conceiving']\n",
    "dimensionality = len(vector_for_example)\n",
    "print(f\"The dimensionality of the GloVe vectors is: {dimensionality}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('Project1-ClassificationDataset.csv')\n",
    "x_train, x_test, y_train, y_test = train_test_split(df['keywords'],df['root_label'], test_size=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast  \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "glove_embeddings = word_embeddings \n",
    "\n",
    "def document_embeddings(document, glove_embeddings):\n",
    "    \n",
    "    word_embeddings = []    \n",
    "    words = ast.literal_eval(document)\n",
    "    # print(f\"words{type(words)}\")\n",
    "    \n",
    "    for word in words:\n",
    "        # print(f\"word: {word}\")\n",
    "        if word in glove_embeddings:\n",
    "            word_embeddings.append(glove_embeddings[word])\n",
    "    \n",
    "    # average word embeddings\n",
    "    avg_embedding = np.mean(normalize(word_embeddings), axis=0)\n",
    "    # print(f\"len:{len(avg_embedding)}\")\n",
    "    return avg_embedding\n",
    "\n",
    "# Generate document embeddings for all documents\n",
    "X_train_embeddings = np.array([document_embeddings(doc, glove_embeddings) for doc in x_train])\n",
    "X_test_embeddings = np.array([document_embeddings(doc, glove_embeddings) for doc in x_test])\n",
    "print(f\"X_train_embeddings{len(X_train_embeddings)}\")\n",
    "\n",
    "\n",
    "# classifier used: Logistic Regression\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train_embeddings, y_train)\n",
    "y_pred = classifier.predict(X_test_embeddings)\n",
    "\n",
    "# Accuracy of the Model  \n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 12\n",
    "Plot the relationship between the dimension of the pre-trained GLoVE embedding and the resulting accuracy of the model in the classification task. Describe the observed trend. Is this trend expected? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "The trend shows that as the dimensions of the pre-trained GLoVE embeddings increases we get better accuracy. Thus the accuracy increses as the dimisions increase.   \n",
    "Yes, this is the expected trend. This is because as the dimesions increases there is more information captured per word, which could improve the classification task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train_and_evaluate(X_train, X_test, y_train, y_test):\n",
    "    classifier = LogisticRegression()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"accuracy:{accuracy}\")\n",
    "    return accuracy\n",
    "\n",
    "# Differnt GLoVE embeddings files\n",
    "embedding_files = ['glove.6B.50d.txt', 'glove.6B.100d.txt', 'glove.6B.200d.txt', 'glove.6B.300d.txt']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['keywords'],df['root_label'], test_size=0.2)\n",
    "\n",
    "accuracies = []\n",
    "embedding_dimensions = []\n",
    "i = 0\n",
    "\n",
    "for file in embedding_files:\n",
    "    \n",
    "    embeddings = load_glove_embeddings(file)\n",
    "    \n",
    "    # Generate document embeddings for training and testing data\n",
    "    X_train_embeddings = np.array([document_embeddings(doc, embeddings) for doc in X_train])\n",
    "    X_test_embeddings = np.array([document_embeddings(doc, embeddings) for doc in X_test])\n",
    "    print(f\"X_train_embeddings: {len(X_train_embeddings[0])}\")\n",
    "    \n",
    "    # Train and evaluate classifier\n",
    "    accuracy = train_and_evaluate(X_train_embeddings, X_test_embeddings, y_train, y_test)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    embedding_dim = i\n",
    "    i += 1\n",
    "    embedding_dimensions.append(embedding_dim)\n",
    "    \n",
    "    print(f\"Embedding Dimension: {embedding_dim}, Accuracy: {accuracy}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 13\n",
    "Compare and contrast the two visualizations. Are there clusters formed in either or both of the plots? We will pursue the clustering aspect further in the next project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "In the 2-D visualization for GLoVE-based embeddings we can see that the two classes seened to be forming clusters. Points that belong to the same class are closer to eachother, this shows that the GLoVE-based embeddings bring similar classes closer to form clusters. Whereas, we can see that the 2-D representation of the normalized random vectors shows the data points scattered all around the two dimensnsional space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import umap.plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_embeddings = np.array([document_embeddings(doc, word_embeddings) for doc in x_train])\n",
    "X_test_embeddings = np.array([document_embeddings(doc, word_embeddings) for doc in x_test])\n",
    "\n",
    "glove_embedding_2d = umap.UMAP(n_components=2, metric='euclidean').fit(X_train_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for normalized random vectors\n",
    "import matplotlib.pyplot as plt\n",
    "random_vectors = np.random.normal(0, 1, (2780, 300))\n",
    "\n",
    "\n",
    "random_vectors_norm = random_vectors / np.linalg.norm(random_vectors, axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "umap_reducer = umap.UMAP(n_components=10, metric='cosine')\n",
    "umap_embeddings = umap_reducer.fit_transform(random_vectors_norm)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(umap_embeddings[:, 0], umap_embeddings[:, 1], s=5)\n",
    "plt.title('2-D Visualization of Normalized Random Vectors')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "glove = umap.plot.points(glove_embedding_2d,labels=np.array(y_train))\n",
    "plt.title('2-D visualization for GLoVE-based embeddings')\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
