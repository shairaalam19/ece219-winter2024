{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Recommendation Systems\n",
    "\n",
    "## Group Members\n",
    "- Shaira Alam\n",
    "\n",
    "- Vani Agrawal \n",
    "\n",
    "- Dhakshina Ilango"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7\n",
    "Understanding the NMF cost function: Is the optimization problem given by equation 5 convex? Consider the optimization problem given by equation 5. For U fixed, formulate it as a least-squares problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 8\n",
    "Designing the NMF Collaborative Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A\n",
    "Design a NMF-based collaborative filter to predict the ratings of the movies in the original dataset and evaluate its performance using 10-fold cross-validation. Sweep k (number of latent factors) from 2 to 50 in step sizes of 2, and for each k compute the average RMSE and average MAE obtained by averaging the RMSE and MAE across all 10 folds. If NMF takes too long, you can increase the step size. Increasing it too much will result in poorer granularity in your results. Plot the average RMSE (Y-axis) against k (X-axis) and the average MAE (Yaxis) against k (X-axis). For solving this question, use the default value for the regularization parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B\n",
    "Use the plot from the previous part to find the optimal number of latent factors. Optimal number of latent factors is the value of k that gives the minimum average RMSE or the minimum average MAE. Please report the minimum average RMSE and MAE. Is the optimal number of latent factors same as the number of movie genres?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C\n",
    "Performance on trimmed dataset subsets: For each of Popular, Unpopular and High-Variance subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Design a NMF collaborative filter for each trimmed subset and evaluate its performance using 10-fold cross validation. Sweep k (number of latent factors) from 2 to 50 in step sizes of 2, and for each k compute the average RMSE obtained by averaging the RMSE across all 10 folds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot average RMSE (Y-axis) against k (X-axis); item Report the minimum average RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D\n",
    "Plot the ROC curves for the NMF-based collaborative filter and also report the area under the curve (AUC) value as done in Question 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 9\n",
    "Interpreting the NMF model: Perform Non-negative matrix factorization on\n",
    "the ratings matrix R to obtain the factor matrices U and V , where U represents the user-latent factors interaction and V represents the movie-latent factors interaction (use k = 20). For each column of V , sort the movies in descending order and report the genres of the top 10 movies. Do the top 10 movies belong to a particular or a small collection of genre? Is there a connection between the latent factors and the movie genres?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 10\n",
    "Designing the MF Collaborative Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A\n",
    "Design a MF-based collaborative filter to predict the ratings of the movies in the original dataset and evaluate it’s performance using 10-fold cross-validation. Sweep k (number of latent factors) from 2 to 50 in step sizes of 2, and for each k compute the average RMSE and average MAE obtained by averaging the RMSE and MAE across all 10 folds. Plot the average RMSE (Y-axis) against k (X-axis) and the average MAE (Y-axis) against k (X-axis). For solving this question, use the default value for the regularization parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B\n",
    "Use the plot from the previous part to find the optimal number of latent factors. Optimal\n",
    "number of latent factors is the value of k that gives the minimum average RMSE or the\n",
    "minimum average MAE. Please report the minimum average RMSE and MAE. Is the optimal\n",
    "number of latent factors same as the number of movie genres?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C\n",
    "Performance on dataset subsets: For each of Popular, Unpopular and High-Variance subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Design a MF collaborative filter for each trimmed subset and evaluate its performance using 10-fold cross validation. Sweep k (number of latent factors) from 2 to 50 in step sizes of 2, and for each k compute the average RMSE obtained by averaging the RMSE across all 10 folds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot average RMSE (Y-axis) against k (X-axis); item Report the minimum average RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the ROC curves for the MF-based collaborative filter and also report the area under the curve (AUC) value as done in Question 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A\n",
    "Design a naive collaborative filter to predict the ratings of the movies in the original dataset and evaluate it’s performance using 10-fold cross validation. Compute the average RMSE by averaging the RMSE across all 10 folds. Report the average RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B\n",
    "Performance on dataset subsets: For each of Popular, Unpopular and High-Variance test subsets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Design a naive collaborative filter for each trimmed set and evaluate its performance using 10-fold cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the average RMSE by averaging the RMSE across all 10 folds. Report the average RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 12 \n",
    "Comparing the most performant models across architecture: Plot the best ROC curves (threshold = 3) for the k-NN, NMF, and MF with bias based  ollaborative filters in the same figure. Use the figure to compare the performance of the filters in predicting the ratings of the movies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 13 \n",
    "Use the provided helper code for loading and preprocessing Web10k data. Print out the number of unique queries in total and show distribution of relevance labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in /Users/vaniagrawal/anaconda3/lib/python3.10/site-packages (4.1.0)\n",
      "Requirement already satisfied: numpy in /Users/vaniagrawal/anaconda3/lib/python3.10/site-packages (from lightgbm) (1.26.3)\n",
      "Requirement already satisfied: scipy in /Users/vaniagrawal/anaconda3/lib/python3.10/site-packages (from lightgbm) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.metrics import ndcg_score\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset for one fold\n",
    "def load_one_fole(data_path):\n",
    "    X_train, y_train, qid_train = load_svmlight_file(str(data_path + 'train.txt'), query_id=True)\n",
    "    X_test, y_test, qid_test = load_svmlight_file(str(data_path + 'test.txt'), query_id=True)\n",
    "    y_train = y_train.astype(int)\n",
    "    y_test = y_test.astype(int)\n",
    "    _, group_train = np.unique(qid_train, return_counts=True)\n",
    "    _, group_test = np.unique(qid_test, return_counts=True)\n",
    "    return X_train, y_train, qid_train, group_train, X_test, y_test, qid_test, group_test\n",
    "\n",
    "def ndcg_single_query(y_score, y_true, k):\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])\n",
    "\n",
    "    gain = 2 ** y_true - 1\n",
    "\n",
    "    discounts = np.log2(np.arange(len(y_true)) + 2)\n",
    "    return np.sum(gain / discounts)\n",
    "\n",
    "# calculate NDCG score given a trained model \n",
    "def compute_ndcg_all(model, X_test, y_test, qids_test, k=10):\n",
    "    unique_qids = np.unique(qids_test)\n",
    "    ndcg_ = list()\n",
    "    for i, qid in enumerate(unique_qids):\n",
    "        y = y_test[qids_test == qid]\n",
    "\n",
    "        if np.sum(y) == 0:\n",
    "            continue\n",
    "\n",
    "        p = model.predict(X_test[qids_test == qid])\n",
    "\n",
    "        idcg = ndcg_single_query(y, y, k=k)\n",
    "        ndcg_.append(ndcg_single_query(p, y, k=k) / idcg)\n",
    "    return np.mean(ndcg_)\n",
    "\n",
    "# get importance of features\n",
    "def get_feature_importance(model, importance_type='gain'):\n",
    "    return model.booster_.feature_importance(importance_type=importance_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold1: val:[0 1 2 3 4] count:[502741 310465 127541  17108   7078]\n",
      "Fold2: val:[0 1 2 3 4] count:[499479 308384 126992  16867   6949]\n",
      "Fold3: val:[0 1 2 3 4] count:[497813 308264 127576  17264   7287]\n",
      "Fold4: val:[0 1 2 3 4] count:[498175 310318 128538  16956   7112]\n",
      "Fold5: val:[0 1 2 3 4] count:[498844 307689 127157  17073   7098]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "datapath = './MSLR-WEB10K'\n",
    "\n",
    "total_counts = np.zeros(5, dtype=int)\n",
    "\n",
    "# Loop through each fold\n",
    "for fold_num in range(1, 6):\n",
    "    # Load data for the current fold\n",
    "    fold_path = os.path.join(datapath, f'Fold{fold_num}/')\n",
    "    _, y_train, _, _, _, y_test, _, _ = load_one_fole(fold_path)\n",
    "\n",
    "    # Calculate and print distribution of relevance labels for the current fold\n",
    "    unique_labels, label_counts = np.unique(np.concatenate([y_train, y_test]), return_counts=True)\n",
    "    print(f'Fold{fold_num}: val:{unique_labels} count:{label_counts}')\n",
    "\n",
    "    # Update total counts\n",
    "    total_counts += label_counts\n",
    "\n",
    "# Combine counts across all folds into a single plot\n",
    "relevance_labels = [0, 1, 2, 3, 4]\n",
    "\n",
    "# Plot the total frequency of relevance labels\n",
    "plt.bar(relevance_labels, total_counts)\n",
    "plt.xlabel('Relevance Label')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Relevance Labels Across Folds')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combined counts across all folds:\n",
      "relevance label:  0                 1                  2                 3               4\n",
      "frequency:      2497052         1545120              637804            85268           35524\n"
     ]
    }
   ],
   "source": [
    "# Print the combined counts\n",
    "print(\"\\nCombined counts across all folds:\")\n",
    "print(\"relevance label:  0                 1                  2                 3               4\")\n",
    "print(f\"frequency:      {total_counts[0]}         {total_counts[1]}              {total_counts[2]}            {total_counts[3]}           {total_counts[4]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 14\n",
    "For each of the five provided folds, train a LightGBM model using the ’lambdarank’ objective. After\n",
    "training, evaluate and report the model’s performance on the test set using nDCG@3, nDCG@5 and\n",
    "nDCG@10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "Fold 1: nDCG@3 = 0.4565, nDCG@5 = 0.4633, nDCG@10 = 0.4829\n",
      "Fold 2: nDCG@3 = 0.4538, nDCG@5 = 0.4572, nDCG@10 = 0.4767\n",
      "Fold 3: nDCG@3 = 0.4491, nDCG@5 = 0.4583, nDCG@10 = 0.4759\n",
      "Fold 4: nDCG@3 = 0.4612, nDCG@5 = 0.4663, nDCG@10 = 0.4877\n",
      "Fold 5: nDCG@3 = 0.4697, nDCG@5 = 0.4715, nDCG@10 = 0.4905\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "datapath = './MSLR-WEB10K'\n",
    "\n",
    "results = []\n",
    "\n",
    "for fold_num in range(1, 6):\n",
    "    # Load data for the current fold\n",
    "    fold_path = os.path.join(datapath, f'Fold{fold_num}/')\n",
    "    X_train, y_train, qid_train, group_train, X_test, y_test, qid_test, group_test = load_one_fole(fold_path)\n",
    "\n",
    "    # LightGBM model with 'lambdarank' objective\n",
    "    params = {'objective': 'lambdarank', 'metric': 'ndcg', 'ndcg_eval_at': [3, 5, 10], 'verbose': 0}\n",
    "    fold_train_data = lgb.Dataset(X_train, label=y_train, group=group_train, free_raw_data=False)\n",
    "    fold_test_data = lgb.Dataset(X_test, label=y_test, group=group_test, free_raw_data=False)\n",
    "\n",
    "    lgb_model = lgb.train(params, fold_train_data, valid_sets=[fold_test_data])\n",
    "\n",
    "    # Evalauting the model on the test set\n",
    "    ndcg3 = compute_ndcg_all(lgb_model, X_test, y_test, qid_test, k=3)\n",
    "    ndcg5 = compute_ndcg_all(lgb_model, X_test, y_test, qid_test, k=5)\n",
    "    ndcg10 = compute_ndcg_all(lgb_model, X_test, y_test, qid_test, k=10)\n",
    "\n",
    "    results.append({'fold': fold_num, 'ndcg@3': ndcg3, 'ndcg@5': ndcg5, 'ndcg@10': ndcg10})\n",
    "\n",
    "print(\"\\nEvaluation Results:\")\n",
    "for r in results:\n",
    "    print(f\"Fold {r['fold']}: nDCG@3 = {r['ndcg@3']:.4f}, nDCG@5 = {r['ndcg@5']:.4f}, nDCG@10 = {r['ndcg@10']:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 15\n",
    "For each of the five provided folds, list top 5 most important features of the model based on\n",
    "the importance score. Please use model.booster .feature importance(importance type=’gain’) as\n",
    "demonstrated here for retrieving importance score per feature. You can also find helper code in the\n",
    "provided notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 features for Fold 1 based on importance score:\n",
      "Feature 134\n",
      "Feature 8\n",
      "Feature 108\n",
      "Feature 55\n",
      "Feature 130\n",
      "\n",
      "Top 5 features for Fold 2 based on importance score:\n",
      "Feature 134\n",
      "Feature 8\n",
      "Feature 55\n",
      "Feature 108\n",
      "Feature 130\n",
      "\n",
      "Top 5 features for Fold 3 based on importance score:\n",
      "Feature 134\n",
      "Feature 55\n",
      "Feature 108\n",
      "Feature 130\n",
      "Feature 8\n",
      "\n",
      "Top 5 features for Fold 4 based on importance score:\n",
      "Feature 134\n",
      "Feature 8\n",
      "Feature 55\n",
      "Feature 130\n",
      "Feature 129\n",
      "\n",
      "Top 5 features for Fold 5 based on importance score:\n",
      "Feature 134\n",
      "Feature 8\n",
      "Feature 55\n",
      "Feature 108\n",
      "Feature 130\n"
     ]
    }
   ],
   "source": [
    "datapath = './MSLR-WEB10K'\n",
    "\n",
    "for fold_num in range(1, 6):\n",
    "    # Load data for the current fold\n",
    "    fold_path = os.path.join(datapath, f'Fold{fold_num}/')\n",
    "    X_train, y_train, qid_train, group_train, X_test, y_test, qid_test, group_test = load_one_fole(fold_path)\n",
    "\n",
    "    # LightGBM model with 'lambdarank' objective\n",
    "    params = {'objective': 'lambdarank', 'metric': 'ndcg', 'ndcg_eval_at': [3, 5, 10], 'verbose': 0}\n",
    "    fold_train_data = lgb.Dataset(X_train, label=y_train, group=group_train, free_raw_data=False)\n",
    "    fold_test_data = lgb.Dataset(X_test, label=y_test, group=group_test, free_raw_data=False)\n",
    "\n",
    "    lgb_model = lgb.train(params, fold_train_data, valid_sets=[fold_test_data])\n",
    "\n",
    "    # Get feature importance\n",
    "    feature_importance = lgb_model.feature_importance(importance_type=\"gain\")\n",
    "\n",
    "    # Find and print the top 5 most important features\n",
    "    top_features_indices = np.argsort(feature_importance)[::-1][:5]\n",
    "    top_features = [f\"Feature {idx + 1}\" for idx in top_features_indices]\n",
    "    \n",
    "    print(f\"\\nTop 5 features for Fold {fold_num} based on importance score:\")\n",
    "    for feature in top_features:\n",
    "        print(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 16.1\n",
    "For each of the five provided folds:\n",
    "- Remove the top 20 most important features according to the computed importance score in\n",
    "the question 15. Then train a new LightGBM model on the resulted 116 dimensional query-\n",
    "url data. Evaluate the performance of this new model on the test set using nDCG. Does\n",
    "the outcome align with your expectations? If not, please share your hypothesis regarding the\n",
    "potential reasons for this discrepancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1 - After removing top 20 features: nDCG@10 = 0.4081\n",
      "\n",
      "Fold 2 - After removing top 20 features: nDCG@10 = 0.4046\n",
      "\n",
      "Fold 3 - After removing top 20 features: nDCG@10 = 0.4114\n",
      "\n",
      "Fold 4 - After removing top 20 features: nDCG@10 = 0.4123\n",
      "\n",
      "Fold 5 - After removing top 20 features: nDCG@10 = 0.4169\n"
     ]
    }
   ],
   "source": [
    "datapath = './MSLR-WEB10K'\n",
    "\n",
    "for fold_num in range(1, 6):\n",
    "    # Load data for the current fold\n",
    "    fold_path = os.path.join(datapath, f'Fold{fold_num}/')\n",
    "    X_train, y_train, qid_train, group_train, X_test, y_test, qid_test, group_test = load_one_fole(fold_path)\n",
    "\n",
    "    # LightGBM model with 'lambdarank' objective\n",
    "    params = {'objective': 'lambdarank', 'metric': 'ndcg', 'ndcg_eval_at': [3, 5, 10], 'verbose': 0}\n",
    "    fold_train_data = lgb.Dataset(X_train, label=y_train, group=group_train, free_raw_data=False)\n",
    "    fold_test_data = lgb.Dataset(X_test, label=y_test, group=group_test, free_raw_data=False)\n",
    "\n",
    "    lgb_model = lgb.train(params, fold_train_data, valid_sets=[fold_test_data])\n",
    "\n",
    "    # Get feature importance\n",
    "    feature_importance = lgb_model.feature_importance(importance_type=\"gain\")\n",
    "\n",
    "    # Get the indices of the top 20 features\n",
    "    top20_indices = np.argsort(feature_importance)[::-1][:20]\n",
    "\n",
    "    # Remove top 20 features\n",
    "    X_train_top20 = np.delete(X_train.toarray(), top20_indices, axis=1)\n",
    "    X_test_top20 = np.delete(X_test.toarray(), top20_indices, axis=1)\n",
    "\n",
    "    model_top20 = lgb.train(params, lgb.Dataset(X_train_top20, label=y_train, group=group_train, free_raw_data=False))\n",
    "\n",
    "    # Evaluate the performance on the test set using nDCG\n",
    "    ndcg_top20 = compute_ndcg_all(model_top20, X_test_top20, y_test, qid_test, k=10)\n",
    "    print(f\"\\nFold {fold_num} - After removing top 20 features: nDCG@10 = {ndcg_top20:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 16.2\n",
    "For each of the five provided folds:\n",
    "- Remove the 60 least important features according to the computed importance score in the\n",
    "question 15. Then train a new LightGBM model on the resulted 76 dimensional query-url data.\n",
    "Evaluate the performance of this new model on the test set using nDCG. Does the outcome\n",
    "align with your expectations? If not, please share your hypothesis regarding the potential\n",
    "reasons for this discrepancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1 - After removing bottom 60 features: nDCG@10 = 0.4821\n",
      "\n",
      "Fold 2 - After removing bottom 60 features: nDCG@10 = 0.4772\n",
      "\n",
      "Fold 3 - After removing bottom 60 features: nDCG@10 = 0.4774\n",
      "\n",
      "Fold 4 - After removing bottom 60 features: nDCG@10 = 0.4889\n",
      "\n",
      "Fold 5 - After removing bottom 60 features: nDCG@10 = 0.4910\n"
     ]
    }
   ],
   "source": [
    "datapath = './MSLR-WEB10K'\n",
    "\n",
    "for fold_num in range(1, 6):\n",
    "    # Load data for the current fold\n",
    "    fold_path = os.path.join(datapath, f'Fold{fold_num}/')\n",
    "    X_train, y_train, qid_train, group_train, X_test, y_test, qid_test, group_test = load_one_fole(fold_path)\n",
    "\n",
    "    # LightGBM model with 'lambdarank' objective\n",
    "    params = {'objective': 'lambdarank', 'metric': 'ndcg', 'ndcg_eval_at': [3, 5, 10], 'verbose': 0}\n",
    "    fold_train_data = lgb.Dataset(X_train, label=y_train, group=group_train, free_raw_data=False)\n",
    "    fold_test_data = lgb.Dataset(X_test, label=y_test, group=group_test, free_raw_data=False)\n",
    "\n",
    "    lgb_model = lgb.train(params, fold_train_data, valid_sets=[fold_test_data])\n",
    "\n",
    "    # Get feature importance\n",
    "    feature_importance = lgb_model.feature_importance(importance_type=\"gain\")\n",
    "\n",
    "    # Get the indices of the bottom 60 features\n",
    "    bottom60_indices = np.argsort(feature_importance)[:60]\n",
    "\n",
    "    # Remove top 20 features\n",
    "    X_train_bottom60 = np.delete(X_train.toarray(), bottom60_indices, axis=1)\n",
    "    X_test_bottom60 = np.delete(X_test.toarray(), bottom60_indices, axis=1)\n",
    "\n",
    "    model_bottom60 = lgb.train(params, lgb.Dataset(X_train_bottom60, label=y_train, group=group_train, free_raw_data=False))\n",
    "\n",
    "    # Evaluate the performance on the test set using nDCG\n",
    "    ndcg_bottom60 = compute_ndcg_all(model_bottom60, X_test_bottom60, y_test, qid_test, k=10)\n",
    "    print(f\"\\nFold {fold_num} - After removing bottom 60 features: nDCG@10 = {ndcg_bottom60:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
